{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13366134,"sourceType":"datasetVersion","datasetId":8478863}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q optuna transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/aml-dataset/augmented_catalog_final_2.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train = df_train.drop('image_link', axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import argparse","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/aml-dataset/augmented_test_final.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== Cell 0: Pin compatible versions (run once, then restart & run from Cell 1) ====\n!pip -q install \"httpx>=0.27.0\" \"huggingface_hub>=0.24.6\" \"transformers>=4.43.4,<4.46\"\n\nimport sys, os, pkgutil\nprint(\"httpx  :\", __import__(\"httpx\").__version__)\nprint(\"hf_hub:\", __import__(\"huggingface_hub\").__version__)\nprint(\"trfs  :\", __import__(\"transformers\").__version__)\n\nprint(\"\\n✅ Please now restart the Python kernel (Kaggle: Runtime > Restart) and run from Cell 1.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===== Cell 1: Imports & Config =====\nimport os\nimport json\nimport math\nimport random\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModel,\n    DataCollatorWithPadding,\n    get_cosine_schedule_with_warmup\n)\nfrom torch.optim import AdamW\nfrom tqdm.auto import tqdm\n\n# Optional (your original had this, but Kaggle has 1 GPU; keep commented)\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nCONFIG = {\n    # Paths\n    \"train_path\": \"/kaggle/input/aml-dataset/augmented_catalog_final_2.csv\",   # change if yours differs\n    \"test_path\":  \"/kaggle/input/aml-dataset/augmented_test_final.csv\",\n    \"final_model_path\": \"/kaggle/working/price_predictor_deberta_v3_large.pt\",\n    \"out_dir\": \"/kaggle/working\",\n\n    # Repro\n    \"random_seed\": 42,\n\n    # Model\n    \"model_name\": \"microsoft/deberta-v3-small\",\n    \"max_length\": 160,   # use for both search & final to stay as close to original as possible\n    \"dropout\": 0.2,      # will be overridden by best search value for final run\n\n    # Training (final)\n    \"batch_size\": 16,\n    \"epochs\": 10,        # your original default for full-data train\n    \"lr_encoder\": 2e-5,  # overridden by best search value\n    \"lr_head\": 1e-3,     # overridden by best search value\n    \"weight_decay\": 0.01,\n    \"warmup_ratio\": 0.1,\n    \"gradient_clip\": 1.0,\n\n    # System\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    \"num_workers\": max(0, min(8, (os.cpu_count() or 2) // 2)),\n    \"use_amp\": True,\n}\n\n# Search settings (simple grid, no extra dependency)\nSEARCH = {\n    \"folds\": 3,\n    \"epochs\": 3,   # keep short to fit 1–2h; adjust if you want more accuracy\n    \"batch_size\": 16,\n    \"grid_lr_encoder\": [2e-5, 3e-5, 5e-5],\n    \"grid_lr_head\":    [1e-3, 2e-3, 3e-3],\n    \"grid_dropout\":    [0.2, 0.3],\n}\n\ndef seed_everything(seed: int = 42):\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n\nseed_everything(CONFIG[\"random_seed\"])\ndevice = torch.device(CONFIG[\"device\"])\nprint(\"Device:\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T12:23:19.320621Z","iopub.execute_input":"2025-10-13T12:23:19.321250Z","iopub.status.idle":"2025-10-13T12:23:25.730486Z","shell.execute_reply.started":"2025-10-13T12:23:19.321221Z","shell.execute_reply":"2025-10-13T12:23:25.729793Z"}},"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a61e3a1db554d928f7b7cfa76ba3347"}},"metadata":{}},{"name":"stderr","text":"2025-10-13 12:23:22.841966: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760358202.864708     129 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760358202.871822     129 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ===== Cell 2: Helpers & original-style components =====\n\ndef create_combined_text(row):\n    category = str(row['product_category']).strip()\n    description = str(row['description']).strip() if 'description' in row and pd.notna(row['description']) else \"\"\n    value = row['value']\n    unit = str(row['unit']).strip().capitalize()\n    text_parts = [f\"Category: {category}\"]\n    if description:\n        text_parts.append(f\"Description: {description}\")\n    if unit.lower() != 'count' or value != 1:\n        text_parts.append(f\"Amount: {value} {unit}\")\n    return \" [SEP] \".join(text_parts)\n\nclass PriceDataset(Dataset):\n    def __init__(self, encodings, prices):\n        self.encodings = encodings\n        self.prices = prices\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])\n    def __getitem__(self, idx):\n        item = {k: self.encodings[k][idx] for k in ('input_ids', 'attention_mask')}\n        if self.prices is not None:\n            item['price'] = torch.tensor(self.prices[idx], dtype=torch.float32)\n        return item\n\nclass PriceRegressor(nn.Module):\n    def __init__(self, model_name: str, dropout: float = 0.2):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(model_name)\n        hidden_size = self.encoder.config.hidden_size\n        self.regressor = nn.Sequential(\n            nn.LayerNorm(hidden_size),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_size, 512),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(512, 256),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(256, 1)\n        )\n    def forward(self, input_ids, attention_mask):\n        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        last_hidden = outputs.last_hidden_state\n        mask = attention_mask.unsqueeze(-1).type_as(last_hidden)\n        pooled = (last_hidden * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-9)\n        return self.regressor(pooled).squeeze(-1)\n\ndef build_optimizer(model, lr_enc, lr_head, weight_decay, fused=True):\n    encoder_params = [p for p in model.encoder.parameters() if p.requires_grad]\n    head_params    = list(model.regressor.parameters())\n    opt_kwargs = {\"weight_decay\": weight_decay}\n    if device.type == \"cuda\" and fused:\n        opt_kwargs[\"fused\"] = True\n    optimizer = AdamW([\n        {'params': encoder_params, 'lr': lr_enc},\n        {'params': head_params,    'lr': lr_head}\n    ], **opt_kwargs)\n    return optimizer\n\ndef build_collate_fn(tokenizer):\n    collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\", return_tensors=\"pt\")\n    def collate_fn(batch):\n        if 'price' in batch[0]:\n            prices = torch.tensor([ex['price'] for ex in batch], dtype=torch.float32)\n        else:\n            prices = None\n        features = [{k: ex[k] for k in ('input_ids', 'attention_mask')} for ex in batch]\n        batch_out = collator(features)\n        if prices is not None:\n            batch_out['price'] = prices\n        return batch_out\n    return collate_fn\n\ndef train_epoch(model, loader, optimizer, scheduler, scaler, amp_enabled):\n    model.train()\n    total_loss = 0.0\n    for batch in tqdm(loader, desc=\"Training\", leave=False):\n        input_ids = batch['input_ids'].to(device, non_blocking=True)\n        attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n        targets = batch['price'].to(device, non_blocking=True)\n\n        optimizer.zero_grad(set_to_none=True)\n        with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=amp_enabled):\n            preds = model(input_ids, attention_mask)           # preds = log_price\n            loss = nn.functional.huber_loss(preds, targets, delta=0.5)\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        nn.utils.clip_grad_norm_(model.parameters(), CONFIG[\"gradient_clip\"])\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n        total_loss += float(loss)\n    return total_loss / max(1, len(loader))\n\n@torch.no_grad()\ndef evaluate_smape(model, loader):\n    model.eval()\n    preds, trues = [], []\n    for batch in loader:\n        input_ids = batch['input_ids'].to(device, non_blocking=True)\n        attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n        targets = batch['price'].to(device, non_blocking=True)\n        out = model(input_ids, attention_mask)    # log_price\n        preds.append(torch.expm1(out).cpu().numpy())\n        trues.append(torch.expm1(targets).cpu().numpy())\n    preds = np.concatenate(preds)\n    trues = np.concatenate(trues)\n    # SMAPE\n    num = np.abs(preds - trues)\n    den = (np.abs(preds) + np.abs(trues) + 0.1) / 2.0\n    return float(np.mean(num / den))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T12:23:33.624245Z","iopub.execute_input":"2025-10-13T12:23:33.624577Z","iopub.status.idle":"2025-10-13T12:23:33.642380Z","shell.execute_reply.started":"2025-10-13T12:23:33.624553Z","shell.execute_reply":"2025-10-13T12:23:33.641509Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ===== Cell 3: Load + preprocess =====\ndef load_train_df(path):\n    df = pd.read_csv(path)\n    if \"description\" not in df.columns and \"misc_info\" in df.columns:\n        df = df.rename(columns={\"misc_info\": \"description\"})\n    # keep your original handling\n    df.dropna(subset=['price', 'product_category'], inplace=True)\n    missing_value_mask = df['value'].isna()\n    df.loc[missing_value_mask, 'value'] = 1\n    df.loc[missing_value_mask, 'unit'] = 'Count'\n    df['description'] = df['description'].fillna(\"\")\n\n    df['combined_text'] = df.apply(create_combined_text, axis=1)\n    df['log_price'] = np.log1p(df['price'])\n    return df.reset_index(drop=True)\n\ndef load_test_df(path):\n    df = pd.read_csv(path)\n    if \"description\" not in df.columns and \"misc_info\" in df.columns:\n        df = df.rename(columns={\"misc_info\": \"description\"})\n    if \"value\" not in df.columns:\n        df[\"value\"] = 1\n    if \"unit\" not in df.columns:\n        df[\"unit\"] = \"Count\"\n    df['description'] = df['description'].fillna(\"\")\n    df['combined_text'] = df.apply(create_combined_text, axis=1)\n    return df.reset_index(drop=True)\n\ndf_train = load_train_df(CONFIG[\"train_path\"])\nprint(\"Train shape:\", df_train.shape)\ndisplay(df_train.head(3))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T12:23:36.505641Z","iopub.execute_input":"2025-10-13T12:23:36.506464Z","iopub.status.idle":"2025-10-13T12:23:38.707097Z","shell.execute_reply.started":"2025-10-13T12:23:36.506409Z","shell.execute_reply":"2025-10-13T12:23:38.706503Z"}},"outputs":[{"name":"stdout","text":"Train shape: (74999, 10)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   sample_id                                   product_category  price  value  \\\n0      33127  La Victoria Green Taco Sauce Mild, 12 Ounce (P...   4.89   72.0   \n1     198967  Salerno Cookies, The Original Butter Cookies, ...  13.12   32.0   \n2     261251  Bear Creek Hearty Soup Bowl, Creamy Chicken wi...   1.97   11.4   \n\n    unit  price_per_unit                                        description  \\\n0  Fl Oz        0.067917                                                      \n1  Ounce        0.410000  Original Butter Cookies: Classic butter cookie...   \n2  Ounce        0.172807  Loaded with hearty long grain wild rice and ve...   \n\n                                          image_link  \\\n0  https://m.media-amazon.com/images/I/51mo8htwTH...   \n1  https://m.media-amazon.com/images/I/71YtriIHAA...   \n2  https://m.media-amazon.com/images/I/51+PFEe-w-...   \n\n                                       combined_text  log_price  \n0  Category: La Victoria Green Taco Sauce Mild, 1...   1.773256  \n1  Category: Salerno Cookies, The Original Butter...   2.647592  \n2  Category: Bear Creek Hearty Soup Bowl, Creamy ...   1.088562  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>product_category</th>\n      <th>price</th>\n      <th>value</th>\n      <th>unit</th>\n      <th>price_per_unit</th>\n      <th>description</th>\n      <th>image_link</th>\n      <th>combined_text</th>\n      <th>log_price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>33127</td>\n      <td>La Victoria Green Taco Sauce Mild, 12 Ounce (P...</td>\n      <td>4.89</td>\n      <td>72.0</td>\n      <td>Fl Oz</td>\n      <td>0.067917</td>\n      <td></td>\n      <td>https://m.media-amazon.com/images/I/51mo8htwTH...</td>\n      <td>Category: La Victoria Green Taco Sauce Mild, 1...</td>\n      <td>1.773256</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>198967</td>\n      <td>Salerno Cookies, The Original Butter Cookies, ...</td>\n      <td>13.12</td>\n      <td>32.0</td>\n      <td>Ounce</td>\n      <td>0.410000</td>\n      <td>Original Butter Cookies: Classic butter cookie...</td>\n      <td>https://m.media-amazon.com/images/I/71YtriIHAA...</td>\n      <td>Category: Salerno Cookies, The Original Butter...</td>\n      <td>2.647592</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>261251</td>\n      <td>Bear Creek Hearty Soup Bowl, Creamy Chicken wi...</td>\n      <td>1.97</td>\n      <td>11.4</td>\n      <td>Ounce</td>\n      <td>0.172807</td>\n      <td>Loaded with hearty long grain wild rice and ve...</td>\n      <td>https://m.media-amazon.com/images/I/51+PFEe-w-...</td>\n      <td>Category: Bear Creek Hearty Soup Bowl, Creamy ...</td>\n      <td>1.088562</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# ===== Cell 4: Tokenizer & fold split (robust) =====\nfrom transformers import AutoTokenizer\nfrom sklearn.model_selection import StratifiedKFold\n\ndef get_tokenizer_robust(model_name: str):\n    # 1) normal path (fast tokenizer)\n    try:\n        print(f\"Loading tokenizer for '{model_name}' (fast)...\")\n        tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n        return tok\n    except Exception as e:\n        print(\"Tokenizer fast load failed. Will try local snapshot. Error:\")\n        print(e)\n\n    # 2) snapshot to a local dir, then load with local_files_only=True\n    try:\n        print(\"Downloading local snapshot and loading tokenizer locally...\")\n        from huggingface_hub import snapshot_download\n        local_dir = snapshot_download(repo_id=model_name)  # downloads tokenizer files\n        tok = AutoTokenizer.from_pretrained(local_dir, use_fast=True, local_files_only=True)\n        return tok\n    except Exception as e2:\n        print(\"Local fast load failed as well. Falling back to slow tokenizer. Error:\")\n        print(e2)\n\n    # 3) last fallback: slow tokenizer\n    print(\"Loading slow tokenizer as last resort...\")\n    tok = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n    return tok\n\ntokenizer = get_tokenizer_robust(CONFIG[\"model_name\"])\n\ndef make_folds(df, n_splits=3, seed=42):\n    df = df.copy()\n    bins = min(50, max(10, int(len(df) ** 0.5)))\n    df[\"price_bin\"] = pd.qcut(df[\"price\"].rank(method=\"first\"), q=bins, labels=False, duplicates=\"drop\")\n    df[\"kfold\"] = -1\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    for fold, (_, val_idx) in enumerate(skf.split(df, df[\"price_bin\"])):\n        df.loc[df.index[val_idx], \"kfold\"] = fold\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T12:23:40.361805Z","iopub.execute_input":"2025-10-13T12:23:40.362103Z","iopub.status.idle":"2025-10-13T12:23:42.937292Z","shell.execute_reply.started":"2025-10-13T12:23:40.362082Z","shell.execute_reply":"2025-10-13T12:23:42.936658Z"}},"outputs":[{"name":"stdout","text":"Loading tokenizer for 'microsoft/deberta-v3-small' (fast)...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:558: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ===== Cell 5: Grid search over (lr_encoder, lr_head, dropout) =====\ndef run_grid_search(df):\n    results = []\n    df_folds = make_folds(df, n_splits=SEARCH[\"folds\"], seed=CONFIG[\"random_seed\"])\n    texts = df_folds[\"combined_text\"].tolist()\n    log_prices = df_folds[\"log_price\"].values\n    # Pre-tokenize once for speed\n    enc = tokenizer(texts, truncation=True, max_length=CONFIG[\"max_length\"], padding=False)\n\n    for dropout in SEARCH[\"grid_dropout\"]:\n        for lr_enc in SEARCH[\"grid_lr_encoder\"]:\n            for lr_head in SEARCH[\"grid_lr_head\"]:\n                fold_scores = []\n                print(f\"\\n>>> Trying: dropout={dropout}, lr_encoder={lr_enc}, lr_head={lr_head}\")\n                for f in range(SEARCH[\"folds\"]):\n                    tr_idx = df_folds.index[df_folds[\"kfold\"] != f].to_list()\n                    va_idx = df_folds.index[df_folds[\"kfold\"] == f].to_list()\n\n                    tr_enc = {k: [enc[k][i] for i in tr_idx] for k in (\"input_ids\",\"attention_mask\")}\n                    va_enc = {k: [enc[k][i] for i in va_idx] for k in (\"input_ids\",\"attention_mask\")}\n                    tr_ds = PriceDataset(tr_enc, log_prices[tr_idx])\n                    va_ds = PriceDataset(va_enc, log_prices[va_idx])\n\n                    collate_fn = build_collate_fn(tokenizer)\n                    common = dict(batch_size=SEARCH[\"batch_size\"], pin_memory=(device.type==\"cuda\"), collate_fn=collate_fn)\n                    if CONFIG[\"num_workers\"] > 0:\n                        common.update(num_workers=CONFIG[\"num_workers\"], persistent_workers=True, prefetch_factor=2)\n                    tr_loader = DataLoader(tr_ds, shuffle=True, **common)\n                    va_loader = DataLoader(va_ds, shuffle=False, **common)\n\n                    # Model & optim\n                    model = PriceRegressor(CONFIG[\"model_name\"], dropout=dropout).to(device)\n                    optimizer = build_optimizer(model, lr_enc, lr_head, CONFIG[\"weight_decay\"], fused=True)\n                    total_steps = len(tr_loader) * SEARCH[\"epochs\"]\n                    warmup_steps = int(total_steps * CONFIG[\"warmup_ratio\"])\n                    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n                    scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\") and CONFIG[\"use_amp\"])\n\n                    # Train few epochs\n                    for ep in range(1, SEARCH[\"epochs\"]+1):\n                        _ = train_epoch(model, tr_loader, optimizer, scheduler, scaler, amp_enabled=(device.type==\"cuda\") and CONFIG[\"use_amp\"])\n\n                    # Validate SMAPE (on price scale)\n                    sm = evaluate_smape(model, va_loader)\n                    fold_scores.append(sm)\n\n                    # cleanup\n                    del model, optimizer, scheduler, scaler, tr_loader, va_loader\n                    torch.cuda.empty_cache(); \n                    import gc; gc.collect()\n\n                mean_smape = float(np.mean(fold_scores))\n                results.append({\n                    \"dropout\": dropout, \"lr_encoder\": lr_enc, \"lr_head\": lr_head,\n                    \"fold_smapes\": fold_scores, \"mean_smape\": mean_smape\n                })\n                print(f\" -> 3-fold SMAPE: {mean_smape:.4f}\")\n\n    res_df = pd.DataFrame(results).sort_values(\"mean_smape\").reset_index(drop=True)\n    res_df.to_csv(os.path.join(CONFIG[\"out_dir\"], \"grid_search_results.csv\"), index=False)\n    return res_df\n\ngrid_results = run_grid_search(df_train)\ndisplay(grid_results.head(10))\nbest_row = grid_results.iloc[0].to_dict()\nbest_params = {\n    \"dropout\": float(best_row[\"dropout\"]),\n    \"lr_encoder\": float(best_row[\"lr_encoder\"]),\n    \"lr_head\": float(best_row[\"lr_head\"])\n}\nprint(\"\\nBest params from grid:\", best_params)\nwith open(os.path.join(CONFIG[\"out_dir\"], \"best_params.json\"), \"w\") as f:\n    json.dump(best_params, f, indent=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T12:25:47.718361Z","iopub.execute_input":"2025-10-13T12:25:47.718990Z"}},"outputs":[{"name":"stdout","text":"\n>>> Trying: dropout=0.2, lr_encoder=2e-05, lr_head=0.001\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_129/612122928.py:37: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\") and CONFIG[\"use_amp\"])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":" -> 3-fold SMAPE: 0.4787\n\n>>> Trying: dropout=0.2, lr_encoder=2e-05, lr_head=0.002\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":" -> 3-fold SMAPE: 0.4776\n\n>>> Trying: dropout=0.2, lr_encoder=2e-05, lr_head=0.003\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":" -> 3-fold SMAPE: 0.4773\n\n>>> Trying: dropout=0.2, lr_encoder=3e-05, lr_head=0.001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":" -> 3-fold SMAPE: 0.4715\n\n>>> Trying: dropout=0.2, lr_encoder=3e-05, lr_head=0.002\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3125 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"860cced9d3b74a6fafbad9fa944b031c"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"print(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===== Cell 6: Final full-data training with best params =====\nCONFIG[\"dropout\"]   = best_params[\"dropout\"]\nCONFIG[\"lr_encoder\"] = best_params[\"lr_encoder\"]\nCONFIG[\"lr_head\"]    = best_params[\"lr_head\"]\n\nprint(\"=\"*70)\nprint(\"Final Model Training on Full Dataset (your original style)\")\nprint(\"=\"*70)\n\n# 1) Data\ntrain_texts = df_train['combined_text'].tolist()\ntrain_prices = df_train['log_price'].values\n\n# 2) Tokenize & dataset\ntrain_enc = tokenizer(train_texts, truncation=True, max_length=CONFIG[\"max_length\"], padding=False)\ntrain_dataset = PriceDataset(train_enc, train_prices)\n\ncollate_fn = build_collate_fn(tokenizer)\nloader_common = dict(batch_size=CONFIG[\"batch_size\"], pin_memory=(device.type==\"cuda\"), collate_fn=collate_fn)\nif CONFIG[\"num_workers\"] > 0:\n    loader_common.update(num_workers=CONFIG[\"num_workers\"], persistent_workers=True, prefetch_factor=2)\ntrain_loader = DataLoader(train_dataset, shuffle=True, **loader_common)\n\n# 3) Build model & optim\nmodel = PriceRegressor(CONFIG[\"model_name\"], dropout=CONFIG[\"dropout\"]).to(device)\noptimizer = build_optimizer(model, CONFIG[\"lr_encoder\"], CONFIG[\"lr_head\"], CONFIG[\"weight_decay\"], fused=True)\ntotal_steps = len(train_loader) * CONFIG[\"epochs\"]\nwarmup_steps = int(total_steps * CONFIG[\"warmup_ratio\"])\nscheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\nscaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\") and CONFIG[\"use_amp\"])\n\n# 4) Train\nfor epoch in range(1, CONFIG[\"epochs\"] + 1):\n    print(f\"\\nEpoch {epoch}/{CONFIG['epochs']}\")\n    train_loss = train_epoch(model, train_loader, optimizer, scheduler, scaler, amp_enabled=(device.type==\"cuda\") and CONFIG[\"use_amp\"])\n    print(f\"  Train Loss (Huber on log_price): {train_loss:.4f}\")\n\n# Save\ntorch.save(model.state_dict(), CONFIG[\"final_model_path\"])\nprint(f\"\\n✅ Final model saved to: {CONFIG['final_model_path']}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===== Cell 7: Predict test & build submission =====\n# Reload model to ensure clean state (optional but safe)\ninfer_model = PriceRegressor(CONFIG[\"model_name\"], dropout=CONFIG[\"dropout\"]).to(device)\ninfer_model.load_state_dict(torch.load(CONFIG[\"final_model_path\"], map_location=device))\ninfer_model.eval()\n\n# Load test\ndf_test = load_test_df(CONFIG[\"test_path\"])\ntest_texts = df_test[\"combined_text\"].tolist()\ntest_enc = tokenizer(test_texts, truncation=True, max_length=CONFIG[\"max_length\"], padding=False)\n\ntest_ds = PriceDataset(test_enc, prices=None)\ndef collate_test(batch):\n    features = [{k: ex[k] for k in ('input_ids', 'attention_mask')} for ex in batch]\n    return DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\", return_tensors=\"pt\")(features)\n\ncommon = dict(batch_size=CONFIG[\"batch_size\"], pin_memory=(device.type==\"cuda\"), collate_fn=collate_test)\nif CONFIG[\"num_workers\"] > 0:\n    common.update(num_workers=CONFIG[\"num_workers\"], persistent_workers=True, prefetch_factor=2)\ntest_loader = DataLoader(test_ds, shuffle=False, **common)\n\n# Predict\nall_preds = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Inference\", leave=False):\n        input_ids = batch['input_ids'].to(device, non_blocking=True)\n        attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n        out = infer_model(input_ids, attention_mask)  # log_price\n        all_preds.append(torch.expm1(out).detach().cpu().numpy())\ntest_price = np.concatenate(all_preds)\n\n# Build submission with columns exactly: id, price\nid_col = \"id\" if \"id\" in df_test.columns else (\"sample_id\" if \"sample_id\" in df_test.columns else None)\nassert id_col is not None, \"Test must contain 'id' or 'sample_id' column.\"\n\nsubmission = pd.DataFrame({\"id\": df_test[id_col].values, \"price\": test_price})\nsub_path = \"/kaggle/working/submission.csv\"\nsubmission.to_csv(sub_path, index=False)\nprint(\"Submission saved at:\", sub_path)\ndisplay(submission.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}