{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13351972,"sourceType":"datasetVersion","datasetId":8468202}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-12T13:34:00.381959Z","iopub.execute_input":"2025-10-12T13:34:00.382250Z","iopub.status.idle":"2025-10-12T13:34:02.797158Z","shell.execute_reply.started":"2025-10-12T13:34:00.382226Z","shell.execute_reply":"2025-10-12T13:34:02.795467Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# pip install -q pandas numpy scikit-learn optuna tensorflow\nimport os, random, math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import mean_absolute_error\nimport tensorflow as tf\nfrom tensorflow.keras import layers, regularizers, callbacks, Model\nimport optuna\n\n# ---------- Config ----------\nDATA_PATH = \"/kaggle/input/training2/train_presentornot.csv\"\nSEED = 42\nVAL_SIZE = 0.2\nTFIDF_MAX_FEATURES = 5000\nCAT_LATENT = 16\nUNIT_LATENT = 8\nDESC_LATENT = 128\nAE_EPOCHS_SMALL = 200\nAE_EPOCHS_TEXT = 200\nAE_BATCH = 512\nAE_PATIENCE = 12\n# ----------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T14:07:43.163586Z","iopub.execute_input":"2025-10-12T14:07:43.164158Z","iopub.status.idle":"2025-10-12T14:07:43.171535Z","shell.execute_reply.started":"2025-10-12T14:07:43.164131Z","shell.execute_reply":"2025-10-12T14:07:43.170411Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\ntf.keras.utils.set_random_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\n\ngpus = tf.config.list_physical_devices(\"GPU\")\nfor g in gpus:\n    try:\n        tf.config.experimental.set_memory_growth(g, True)\n    except:\n        pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T14:07:47.945457Z","iopub.execute_input":"2025-10-12T14:07:47.946056Z","iopub.status.idle":"2025-10-12T14:07:47.957292Z","shell.execute_reply.started":"2025-10-12T14:07:47.946020Z","shell.execute_reply":"2025-10-12T14:07:47.956029Z"}},"outputs":[{"name":"stderr","text":"2025-10-12 14:07:47.951099: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def load_and_prepare(path: str) -> pd.DataFrame:\n    df = pd.read_csv(path)\n    df[\"present\"] = df.get(\"present\", 0).fillna(0).astype(int)\n    df[\"Description\"] = df[\"Description\"].astype(str).where(df[\"present\"] == 0, \"no extra description\")\n    df[\"Description\"] = df[\"Description\"].fillna(\"no extra description\")\n    df[\"product_category\"] = df[\"product_category\"].astype(str).fillna(\"unknown\")\n    df[\"unit\"] = df[\"unit\"].astype(str).fillna(\"unknown\")\n    df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n    df[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n    df[\"value\"] = df[\"value\"].fillna(df[\"value\"].median())\n    df = df[df[\"price\"].notnull()].reset_index(drop=True)\n    return df\n\ndef dense_autoencoder(input_dim: int, latent_dim: int, binary_out: bool, noise=0.05, l2=1e-6):\n    inp = layers.Input(shape=(input_dim,))\n    x = layers.GaussianNoise(noise)(inp)\n    x = layers.Dense(max(64, latent_dim * 2), activation=\"relu\", kernel_regularizer=regularizers.l2(l2))(x)\n    z = layers.Dense(latent_dim, activation=\"linear\", name=\"latent\")(x)\n    x = layers.Dense(max(64, latent_dim * 2), activation=\"relu\")(z)\n    out_act = \"sigmoid\" if binary_out else \"linear\"\n    out = layers.Dense(input_dim, activation=out_act)(x)\n    model = Model(inp, out)\n    enc = Model(inp, z)\n    loss = \"binary_crossentropy\" if binary_out else \"mse\"\n    model.compile(optimizer=\"adam\", loss=loss)\n    return model, enc\n\ndef fit_autoencoder(X_train, X_val, latent_dim, binary_out, epochs, batch, patience, noise=0.05, l2=1e-6):\n    model, enc = dense_autoencoder(X_train.shape[1], latent_dim, binary_out, noise, l2)\n    es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=patience, restore_best_weights=True, verbose=0)\n    model.fit(X_train, X_train, validation_data=(X_val, X_val),\n              epochs=epochs, batch_size=batch, callbacks=[es], verbose=0)\n    return enc\n\ndef build_feature_blocks(train_df, val_df):\n    ohe_cat = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n    ohe_unit = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n    tfidf = TfidfVectorizer(max_features=TFIDF_MAX_FEATURES, ngram_range=(1, 2), min_df=2)\n\n    X_cat_tr = ohe_cat.fit_transform(train_df[[\"product_category\"]]).astype(\"float32\")\n    X_cat_va = ohe_cat.transform(val_df[[\"product_category\"]]).astype(\"float32\")\n\n    X_unit_tr = ohe_unit.fit_transform(train_df[[\"unit\"]]).astype(\"float32\")\n    X_unit_va = ohe_unit.transform(val_df[[\"unit\"]]).astype(\"float32\")\n\n    X_desc_tr = tfidf.fit_transform(train_df[\"Description\"]).astype(\"float32\").toarray()\n    X_desc_va = tfidf.transform(val_df[\"Description\"]).astype(\"float32\").toarray()\n\n    val_scaler = StandardScaler()\n    v_tr = val_scaler.fit_transform(train_df[[\"value\"]].astype(\"float32\"))\n    v_va = val_scaler.transform(val_df[[\"value\"]].astype(\"float32\"))\n\n    blocks = {\n        \"ohe_cat\": ohe_cat, \"ohe_unit\": ohe_unit, \"tfidf\": tfidf, \"val_scaler\": val_scaler,\n        \"X_cat_tr\": X_cat_tr, \"X_cat_va\": X_cat_va,\n        \"X_unit_tr\": X_unit_tr, \"X_unit_va\": X_unit_va,\n        \"X_desc_tr\": X_desc_tr, \"X_desc_va\": X_desc_va,\n        \"v_tr\": v_tr, \"v_va\": v_va\n    }\n    return blocks\n\ndef encode_blocks(blocks):\n    enc_cat = fit_autoencoder(blocks[\"X_cat_tr\"], blocks[\"X_cat_va\"],\n                              latent_dim=min(CAT_LATENT, max(2, blocks[\"X_cat_tr\"].shape[1] // 2)),\n                              binary_out=True, epochs=AE_EPOCHS_SMALL, batch=AE_BATCH, patience=AE_PATIENCE, noise=0.02)\n    enc_unit = fit_autoencoder(blocks[\"X_unit_tr\"], blocks[\"X_unit_va\"],\n                               latent_dim=min(UNIT_LATENT, max(2, blocks[\"X_unit_tr\"].shape[1] // 2)),\n                               binary_out=True, epochs=AE_EPOCHS_SMALL, batch=AE_BATCH, patience=AE_PATIENCE, noise=0.02)\n    enc_desc = fit_autoencoder(blocks[\"X_desc_tr\"], blocks[\"X_desc_va\"],\n                               latent_dim=DESC_LATENT, binary_out=False,\n                               epochs=AE_EPOCHS_TEXT, batch=min(512, max(64, blocks[\"X_desc_tr\"].shape[0] // 20)),\n                               patience=AE_PATIENCE, noise=0.05, l2=1e-6)\n\n    z_cat_tr = enc_cat.predict(blocks[\"X_cat_tr\"], batch_size=1024, verbose=0)\n    z_cat_va = enc_cat.predict(blocks[\"X_cat_va\"], batch_size=1024, verbose=0)\n    z_unit_tr = enc_unit.predict(blocks[\"X_unit_tr\"], batch_size=1024, verbose=0)\n    z_unit_va = enc_unit.predict(blocks[\"X_unit_va\"], batch_size=1024, verbose=0)\n    z_desc_tr = enc_desc.predict(blocks[\"X_desc_tr\"], batch_size=256, verbose=0)\n    z_desc_va = enc_desc.predict(blocks[\"X_desc_va\"], batch_size=256, verbose=0)\n\n    X_tr = np.hstack([z_cat_tr, z_unit_tr, z_desc_tr, blocks[\"v_tr\"]]).astype(\"float32\")\n    X_va = np.hstack([z_cat_va, z_unit_va, z_desc_va, blocks[\"v_va\"]]).astype(\"float32\")\n\n    encoders = {\"enc_cat\": enc_cat, \"enc_unit\": enc_unit, \"enc_desc\": enc_desc}\n    return X_tr, X_va, encoders\n\ndef build_regressor(input_dim: int, trial: optuna.trial.Trial) -> tf.keras.Model:\n    tf.keras.backend.clear_session()\n    act = trial.suggest_categorical(\"activation\", [\"relu\", \"swish\", \"gelu\"])\n    layers_n = trial.suggest_int(\"layers\", 2, 5)\n    width = trial.suggest_categorical(\"width\", [128, 192, 256, 384, 512, 768, 1024])\n    dropout = trial.suggest_float(\"dropout\", 0.0, 0.35)\n    l2_w = trial.suggest_float(\"l2\", 1e-7, 5e-4, log=True)\n    ln = trial.suggest_categorical(\"norm\", [\"batch\", \"layer\"])\n    lr = trial.suggest_float(\"lr\", 1e-4, 3e-3, log=True)\n\n    inp = layers.Input(shape=(input_dim,))\n    x = layers.Dense(width, activation=act, kernel_regularizer=regularizers.l2(l2_w))(inp)\n    x = layers.Dropout(dropout)(x)\n    for i in range(layers_n - 1):\n        x = layers.Dense(int(width / (1.2 ** (i + 1))), activation=act,\n                         kernel_regularizer=regularizers.l2(l2_w))(x)\n        x = layers.Dropout(dropout)(x)\n        x = (layers.BatchNormalization() if ln == \"batch\" else layers.LayerNormalization())(x)\n    out = layers.Dense(1, activation=\"linear\")(x)\n    model = Model(inp, out)\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n                  loss=tf.keras.losses.Huber(delta=1.0), metrics=[\"mae\"])\n    return model\n\ndef tune_and_train(X_tr, y_tr, X_va, y_va, n_trials=25):\n    def objective(trial):\n        model = build_regressor(X_tr.shape[1], trial)\n        es = callbacks.EarlyStopping(monitor=\"val_mae\", patience=12, restore_best_weights=True, verbose=0)\n        model.fit(X_tr, y_tr, validation_data=(X_va, y_va),\n                  epochs=300, batch_size=trial.suggest_categorical(\"batch\", [64, 128, 256, 512]),\n                  callbacks=[es], verbose=0)\n        pred = model.predict(X_va, batch_size=1024, verbose=0).ravel()\n        return mean_absolute_error(y_va, pred)\n\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n    best_model = build_regressor(X_tr.shape[1], study.best_trial)\n    es = callbacks.EarlyStopping(monitor=\"val_mae\", patience=16, restore_best_weights=True, verbose=0)\n    best_model.fit(X_tr, y_tr, validation_data=(X_va, y_va),\n                   epochs=500, batch_size=study.best_trial.params.get(\"batch\", 256),\n                   callbacks=[es], verbose=0)\n    return study, best_model\n\ndef main():\n    df = load_and_prepare(DATA_PATH)\n    train_df, val_df = train_test_split(df, test_size=VAL_SIZE, random_state=SEED)\n    blocks = build_feature_blocks(train_df, val_df)\n    X_tr, X_va, encoders = encode_blocks(blocks)\n\n    y_tr = train_df[\"price\"].values.astype(\"float32\")\n    y_va = val_df[\"price\"].values.astype(\"float32\")\n\n    study, model = tune_and_train(X_tr, y_tr, X_va, y_va, n_trials=30)\n\n    va_pred = model.predict(X_va, batch_size=1024, verbose=0).ravel()\n    mae = mean_absolute_error(y_va, va_pred)\n    print(f\"Validation MAE: {mae:,.4f}\")\n    print(\"Best params:\", study.best_trial.params)\n\n    os.makedirs(\"artifacts\", exist_ok=True)\n    model.save(\"artifacts/best_regressor.keras\")\n    encoders[\"enc_cat\"].save(\"artifacts/enc_cat.keras\")\n    encoders[\"enc_unit\"].save(\"artifacts/enc_unit.keras\")\n    encoders[\"enc_desc\"].save(\"artifacts/enc_desc.keras\")\n    pd.Series(va_pred, index=val_df.index, name=\"price_pred\").to_csv(\"artifacts/val_predictions.csv\")\n    import joblib\n    joblib.dump(blocks[\"ohe_cat\"], \"artifacts/ohe_cat.joblib\")\n    joblib.dump(blocks[\"ohe_unit\"], \"artifacts/ohe_unit.joblib\")\n    joblib.dump(blocks[\"tfidf\"], \"artifacts/tfidf.joblib\")\n    joblib.dump(blocks[\"val_scaler\"], \"artifacts/value_scaler.joblib\")\n    print(\"Saved artifacts/\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T14:02:44.753656Z","iopub.execute_input":"2025-10-12T14:02:44.753981Z","execution_failed":"2025-10-12T14:03:35.370Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Kaggle-ready: place this in a single cell. Assumes a train.csv with the specified columns.\nimport os, random, json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import mean_absolute_error\nimport tensorflow as tf\nfrom tensorflow.keras import layers, regularizers, callbacks, Model\nimport optuna\nimport xgboost as xgb\nimport joblib\n\n# ---------------- Config ----------------\nDATA_PATH = \"/kaggle/input/training2/train_presentornot.csv\"  # adjust if your file lives under /kaggle/input/<dataset>/train.csv\nSEED = 42\nVAL_SIZE = 0.2\nTFIDF_MAX_FEATURES = 5000\nCAT_LATENT = 16\nUNIT_LATENT = 8\nDESC_LATENT = 128\nAE_EPOCHS_SMALL = 200\nAE_EPOCHS_TEXT = 200\nAE_BATCH = 512\nAE_PATIENCE = 12\nN_TRIALS = 40\n# ---------------------------------------\n\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\nnp.random.seed(SEED); random.seed(SEED); tf.keras.utils.set_random_seed(SEED)\n\ngpus = tf.config.list_physical_devices(\"GPU\")\nfor g in gpus:\n    try:\n        tf.config.experimental.set_memory_growth(g, True)\n    except:\n        pass\n\ndef load_and_prepare(path: str) -> pd.DataFrame:\n    df = pd.read_csv(path)\n    df[\"present\"] = df.get(\"present\", 0).fillna(0).astype(int)\n    df[\"Description\"] = df[\"Description\"].astype(str).where(df[\"present\"] == 0, \"no extra description\")\n    df[\"Description\"] = df[\"Description\"].fillna(\"no extra description\")\n    df[\"product_category\"] = df[\"product_category\"].astype(str).fillna(\"unknown\")\n    df[\"unit\"] = df[\"unit\"].astype(str).fillna(\"unknown\")\n    df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\").fillna(df[\"value\"].median())\n    df[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n    df = df[df[\"price\"].notnull()].reset_index(drop=True)\n    return df\n\ndef dense_autoencoder(input_dim: int, latent_dim: int, binary_out: bool, noise=0.05, l2=1e-6):\n    inp = layers.Input(shape=(input_dim,))\n    x = layers.GaussianNoise(noise)(inp)\n    x = layers.Dense(max(64, latent_dim * 2), activation=\"relu\", kernel_regularizer=regularizers.l2(l2))(x)\n    z = layers.Dense(latent_dim, activation=\"linear\", name=\"latent\")(x)\n    x = layers.Dense(max(64, latent_dim * 2), activation=\"relu\")(z)\n    out = layers.Dense(input_dim, activation=\"sigmoid\" if binary_out else \"linear\")(x)\n    model = Model(inp, out); enc = Model(inp, z)\n    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\" if binary_out else \"mse\")\n    return model, enc\n\ndef fit_autoencoder(X_train, X_val, latent_dim, binary_out, epochs, batch, patience, noise=0.05, l2=1e-6):\n    model, enc = dense_autoencoder(X_train.shape[1], latent_dim, binary_out, noise, l2)\n    es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=patience, restore_best_weights=True, verbose=0)\n    model.fit(X_train, X_train, validation_data=(X_val, X_val),\n              epochs=epochs, batch_size=batch, callbacks=[es], verbose=0)\n    return enc\n\ndef build_feature_blocks(train_df, val_df):\n    ohe_cat = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n    ohe_unit = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n    tfidf = TfidfVectorizer(max_features=TFIDF_MAX_FEATURES, ngram_range=(1, 2), min_df=2)\n\n    X_cat_tr = ohe_cat.fit_transform(train_df[[\"product_category\"]]).astype(\"float32\")\n    X_cat_va = ohe_cat.transform(val_df[[\"product_category\"]]).astype(\"float32\")\n\n    X_unit_tr = ohe_unit.fit_transform(train_df[[\"unit\"]]).astype(\"float32\")\n    X_unit_va = ohe_unit.transform(val_df[[\"unit\"]]).astype(\"float32\")\n\n    X_desc_tr = tfidf.fit_transform(train_df[\"Description\"]).astype(\"float32\").toarray()\n    X_desc_va = tfidf.transform(val_df[\"Description\"]).astype(\"float32\").toarray()\n\n    val_scaler = StandardScaler()\n    v_tr = val_scaler.fit_transform(train_df[[\"value\"]].astype(\"float32\"))\n    v_va = val_scaler.transform(val_df[[\"value\"]].astype(\"float32\"))\n\n    return {\n        \"ohe_cat\": ohe_cat, \"ohe_unit\": ohe_unit, \"tfidf\": tfidf, \"val_scaler\": val_scaler,\n        \"X_cat_tr\": X_cat_tr, \"X_cat_va\": X_cat_va,\n        \"X_unit_tr\": X_unit_tr, \"X_unit_va\": X_unit_va,\n        \"X_desc_tr\": X_desc_tr, \"X_desc_va\": X_desc_va,\n        \"v_tr\": v_tr, \"v_va\": v_va\n    }\n\ndef encode_blocks(blocks):\n    enc_cat = fit_autoencoder(blocks[\"X_cat_tr\"], blocks[\"X_cat_va\"],\n                              latent_dim=min(CAT_LATENT, max(2, blocks[\"X_cat_tr\"].shape[1] // 2)),\n                              binary_out=True, epochs=AE_EPOCHS_SMALL, batch=AE_BATCH, patience=AE_PATIENCE, noise=0.02)\n    enc_unit = fit_autoencoder(blocks[\"X_unit_tr\"], blocks[\"X_unit_va\"],\n                               latent_dim=min(UNIT_LATENT, max(2, blocks[\"X_unit_tr\"].shape[1] // 2)),\n                               binary_out=True, epochs=AE_EPOCHS_SMALL, batch=AE_BATCH, patience=AE_PATIENCE, noise=0.02)\n    enc_desc = fit_autoencoder(blocks[\"X_desc_tr\"], blocks[\"X_desc_va\"],\n                               latent_dim=DESC_LATENT, binary_out=False,\n                               epochs=AE_EPOCHS_TEXT, batch=min(512, max(64, blocks[\"X_desc_tr\"].shape[0] // 20)),\n                               patience=AE_PATIENCE, noise=0.05, l2=1e-6)\n\n    z_cat_tr = enc_cat.predict(blocks[\"X_cat_tr\"], batch_size=1024, verbose=0)\n    z_cat_va = enc_cat.predict(blocks[\"X_cat_va\"], batch_size=1024, verbose=0)\n    z_unit_tr = enc_unit.predict(blocks[\"X_unit_tr\"], batch_size=1024, verbose=0)\n    z_unit_va = enc_unit.predict(blocks[\"X_unit_va\"], batch_size=1024, verbose=0)\n    z_desc_tr = enc_desc.predict(blocks[\"X_desc_tr\"], batch_size=256, verbose=0)\n    z_desc_va = enc_desc.predict(blocks[\"X_desc_va\"], batch_size=256, verbose=0)\n\n    X_tr = np.hstack([z_cat_tr, z_unit_tr, z_desc_tr, blocks[\"v_tr\"]]).astype(\"float32\")\n    X_va = np.hstack([z_cat_va, z_unit_va, z_desc_va, blocks[\"v_va\"]]).astype(\"float32\")\n    return X_tr, X_va, {\"enc_cat\": enc_cat, \"enc_unit\": enc_unit, \"enc_desc\": enc_desc}\n\ndef build_xgb_params(trial, use_gpu: bool):\n    params = {\n        \"objective\": \"reg:squarederror\",\n        \"learning_rate\": trial.suggest_float(\"eta\", 1e-3, 0.3, log=True),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-2, 10.0, log=True),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n        \"reg_alpha\": trial.suggest_float(\"alpha\", 1e-8, 10.0, log=True),\n        \"reg_lambda\": trial.suggest_float(\"lambda\", 1e-6, 10.0, log=True),\n        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n        \"eval_metric\": \"mae\",\n        \"random_state\": SEED,\n        \"n_jobs\": -1,\n        \"tree_method\": \"gpu_hist\" if use_gpu else \"hist\"\n    }\n    return params\n\ndef tune_and_train_xgb(X_tr, y_tr, X_va, y_va, n_trials=N_TRIALS):\n    use_gpu = len(tf.config.list_physical_devices(\"GPU\")) > 0\n    def objective(trial):\n        params = build_xgb_params(trial, use_gpu)\n        n_estimators = trial.suggest_int(\"n_estimators\", 300, 3000)\n        esr = trial.suggest_int(\"early_stopping_rounds\", 50, 200)\n        model = xgb.XGBRegressor(**params, n_estimators=n_estimators, verbosity=0)\n        try:\n            model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False, early_stopping_rounds=esr)\n        except xgb.core.XGBoostError:\n            params[\"tree_method\"] = \"hist\"\n            model = xgb.XGBRegressor(**params, n_estimators=n_estimators, verbosity=0)\n            model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False, early_stopping_rounds=esr)\n        pred = model.predict(X_va)\n        return mean_absolute_error(y_va, pred)\n\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n    best = build_xgb_params(study.best_trial, use_gpu)\n    model = xgb.XGBRegressor(**best, n_estimators=study.best_trial.params[\"n_estimators\"], verbosity=0)\n    try:\n        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False,\n                  early_stopping_rounds=study.best_trial.params[\"early_stopping_rounds\"])\n    except xgb.core.XGBoostError:\n        model.set_params(tree_method=\"hist\")\n        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False,\n                  early_stopping_rounds=study.best_trial.params[\"early_stopping_rounds\"])\n    return study, model\n\ndef main():\n    df = load_and_prepare(DATA_PATH)\n    train_df, val_df = train_test_split(df, test_size=VAL_SIZE, random_state=SEED)\n    blocks = build_feature_blocks(train_df, val_df)\n    X_tr, X_va, encoders = encode_blocks(blocks)\n    y_tr = train_df[\"price\"].values.astype(\"float32\")\n    y_va = val_df[\"price\"].values.astype(\"float32\")\n\n    study, model = tune_and_train_xgb(X_tr, y_tr, X_va, y_va)\n    va_pred = model.predict(X_va)\n    mae = mean_absolute_error(y_va, va_pred)\n    print(f\"Validation MAE: {mae:,.4f}\")\n    print(\"Best params:\", study.best_trial.params)\n\n    os.makedirs(\"artifacts\", exist_ok=True)\n    model.save_model(\"artifacts/xgb_model.json\")\n    encoders[\"enc_cat\"].save(\"artifacts/enc_cat.keras\")\n    encoders[\"enc_unit\"].save(\"artifacts/enc_unit.keras\")\n    encoders[\"enc_desc\"].save(\"artifacts/enc_desc.keras\")\n    joblib.dump(blocks[\"ohe_cat\"], \"artifacts/ohe_cat.joblib\")\n    joblib.dump(blocks[\"ohe_unit\"], \"artifacts/ohe_unit.joblib\")\n    joblib.dump(blocks[\"tfidf\"], \"artifacts/tfidf.joblib\")\n    joblib.dump(blocks[\"val_scaler\"], \"artifacts/value_scaler.joblib\")\n    pd.Series(va_pred, index=val_df.index, name=\"price_pred\").to_csv(\"artifacts/val_predictions_xgb.csv\")\n    with open(\"artifacts/optuna_best_params.json\", \"w\") as f:\n        json.dump(study.best_trial.params, f, indent=2)\n    print(\"Saved to artifacts/\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T14:27:00.474748Z","iopub.execute_input":"2025-10-12T14:27:00.477124Z","execution_failed":"2025-10-12T14:27:50.462Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}