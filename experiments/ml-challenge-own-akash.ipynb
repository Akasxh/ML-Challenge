{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13341039,"sourceType":"datasetVersion","datasetId":8460010},{"sourceId":13345357,"sourceType":"datasetVersion","datasetId":8462978},{"sourceId":13353690,"sourceType":"datasetVersion","datasetId":8469522}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"print(5)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:09:31.130729Z","iopub.execute_input":"2025-10-12T11:09:31.131090Z","iopub.status.idle":"2025-10-12T11:09:31.135266Z","shell.execute_reply.started":"2025-10-12T11:09:31.131067Z","shell.execute_reply":"2025-10-12T11:09:31.134477Z"}},"outputs":[{"name":"stdout","text":"5\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"\n!pip install -U unidecode rapidfuzz emoji pyarrow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:09:31.136969Z","iopub.execute_input":"2025-10-12T11:09:31.137504Z","iopub.status.idle":"2025-10-12T11:09:35.805433Z","shell.execute_reply.started":"2025-10-12T11:09:31.137486Z","shell.execute_reply":"2025-10-12T11:09:35.804714Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: unidecode in /usr/local/lib/python3.11/dist-packages (1.4.0)\nRequirement already satisfied: rapidfuzz in /usr/local/lib/python3.11/dist-packages (3.14.1)\nRequirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (2.15.0)\nRequirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (21.0.0)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import os, re, sys, math, json, gc, string, hashlib, textwrap, unicodedata\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nfrom collections import Counter, defaultdict\nfrom unidecode import unidecode\nfrom rapidfuzz import fuzz\nfrom tqdm.auto import tqdm\nimport emoji\n\n# Reproducibility\nSEED = 42\nnp.random.seed(SEED)\n\n# Paths\nWORK_DIR  = Path(\"/kaggle/working\")\nCACHE_DIR = WORK_DIR / \"cache\"\nCACHE_DIR.mkdir(parents=True, exist_ok=True)\n\ndef _auto_find_file(fname=\"train.csv\"):\n    base = Path(\"/kaggle/input\")\n    candidates = []\n    for p, d, files in os.walk(base):\n        if fname in files:\n            candidates.append(Path(p)/fname)\n    return candidates\n\nFOUND_TRAIN = _auto_find_file(\"train.csv\")\nFOUND_TEST  = _auto_find_file(\"test.csv\")\n\nif len(FOUND_TRAIN)==0 or len(FOUND_TEST)==0:\n    raise FileNotFoundError(\"Could not auto-discover train.csv/test.csv under /kaggle/input. \"\n                            \"Add your dataset to the notebook and re-run.\")\nTRAIN_PATH = FOUND_TRAIN[0]\nTEST_PATH  = FOUND_TEST[0]\n\nprint(\"Using files:\")\nprint(\"  TRAIN:\", TRAIN_PATH)\nprint(\"  TEST :\", TEST_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:09:35.807042Z","iopub.execute_input":"2025-10-12T11:09:35.807285Z","iopub.status.idle":"2025-10-12T11:09:35.827971Z","shell.execute_reply.started":"2025-10-12T11:09:35.807264Z","shell.execute_reply":"2025-10-12T11:09:35.827380Z"}},"outputs":[{"name":"stdout","text":"Using files:\n  TRAIN: /kaggle/input/dataset-products/train.csv\n  TEST : /kaggle/input/dataset-products/test.csv\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"train = pd.read_csv(TRAIN_PATH)\ntest  = pd.read_csv(TEST_PATH)\n\nexpected_cols_train = {\"sample_id\",\"catalog_content\",\"image_link\",\"price\"}\nexpected_cols_test  = {\"sample_id\",\"catalog_content\",\"image_link\"}\n\nassert expected_cols_train.issubset(set(train.columns)), f\"train missing required columns: {expected_cols_train - set(train.columns)}\"\nassert expected_cols_test.issubset(set(test.columns)),   f\"test missing required columns:  {expected_cols_test  - set(test.columns)}\"\n\n# Basic integrity\nassert train[\"sample_id\"].is_unique, \"train.sample_id not unique\"\nassert test[\"sample_id\"].is_unique,  \"test.sample_id not unique\"\nassert (~train[\"price\"].isna()).all(), \"price has NaNs in train\"\nassert (train[\"price\"] > 0).all(), \"price must be positive\"\n\nprint(train.shape, test.shape)\ntrain.head(3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:09:35.828669Z","iopub.execute_input":"2025-10-12T11:09:35.828832Z","iopub.status.idle":"2025-10-12T11:09:37.813644Z","shell.execute_reply.started":"2025-10-12T11:09:35.828819Z","shell.execute_reply":"2025-10-12T11:09:37.812811Z"}},"outputs":[{"name":"stdout","text":"(75000, 4) (75000, 3)\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"   sample_id                                    catalog_content  \\\n0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n1     198967  Item Name: Salerno Cookies, The Original Butte...   \n2     261251  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n\n                                          image_link  price  \n0  https://m.media-amazon.com/images/I/51mo8htwTH...   4.89  \n1  https://m.media-amazon.com/images/I/71YtriIHAA...  13.12  \n2  https://m.media-amazon.com/images/I/51+PFEe-w-...   1.97  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>catalog_content</th>\n      <th>image_link</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>33127</td>\n      <td>Item Name: La Victoria Green Taco Sauce Mild, ...</td>\n      <td>https://m.media-amazon.com/images/I/51mo8htwTH...</td>\n      <td>4.89</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>198967</td>\n      <td>Item Name: Salerno Cookies, The Original Butte...</td>\n      <td>https://m.media-amazon.com/images/I/71YtriIHAA...</td>\n      <td>13.12</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>261251</td>\n      <td>Item Name: Bear Creek Hearty Soup Bowl, Creamy...</td>\n      <td>https://m.media-amazon.com/images/I/51+PFEe-w-...</td>\n      <td>1.97</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"def smape(y_true, y_pred, eps=1e-9):\n    num = np.abs(y_pred - y_true)\n    den = (np.abs(y_true) + np.abs(y_pred) + eps)/2.0\n    return 100.0 * np.mean(num/den)\n\ndef to_ascii(s: str) -> str:\n    s = s or \"\"\n    s = unidecode(str(s))\n    # Keep emojis out; they often confuse regex and add noise\n    s = emoji.replace_emoji(s, replace=\"\")\n    # Normalize whitespace\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    return s\n\ndef normalize_text_for_parsing(s: str) -> str:\n    s = to_ascii(s)\n    # Lower for robust regex; we’ll keep original for brand extraction\n    return s.lower()\n\ndef only_words(s: str) -> str:\n    return re.sub(r\"[^a-z0-9\\s\\-\\.x%/]+\", \" \", s.lower()).strip()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:09:37.815427Z","iopub.execute_input":"2025-10-12T11:09:37.815822Z","iopub.status.idle":"2025-10-12T11:09:37.820672Z","shell.execute_reply.started":"2025-10-12T11:09:37.815795Z","shell.execute_reply":"2025-10-12T11:09:37.820105Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Unit canonicalization maps\nWEIGHT_UNITS = {\n    \"g\": (\"g\", 1.0),\n    \"gram\": (\"g\", 1.0),\n    \"grams\": (\"g\", 1.0),\n    \"kg\": (\"g\", 1000.0),\n    \"kilogram\": (\"g\", 1000.0),\n    \"kilograms\": (\"g\", 1000.0),\n    \"oz\": (\"g\", 28.3495),\n    \"ounce\": (\"g\", 28.3495),\n    \"ounces\": (\"g\", 28.3495),\n    \"lb\": (\"g\", 453.592),\n    \"lbs\": (\"g\", 453.592),\n    \"pound\": (\"g\", 453.592),\n    \"pounds\": (\"g\", 453.592),\n}\n\nVOLUME_UNITS = {\n    \"ml\": (\"ml\", 1.0),\n    \"milliliter\": (\"ml\", 1.0),\n    \"milliliters\": (\"ml\", 1.0),\n    \"l\": (\"ml\", 1000.0),\n    \"liter\": (\"ml\", 1000.0),\n    \"liters\": (\"ml\", 1000.0),\n    \"litre\": (\"ml\", 1000.0),\n    \"litres\": (\"ml\", 1000.0),\n    \"fl oz\": (\"ml\", 29.5735),\n    \"floz\": (\"ml\", 29.5735),\n    \"fluid ounce\": (\"ml\", 29.5735),\n    \"fluid ounces\": (\"ml\", 29.5735),\n}\n\nCOUNT_TOKENS = {\"count\",\"ct\",\"cts\",\"pk\",\"pack\",\"packs\",\"pcs\",\"pieces\",\"tabs\",\"tablets\",\"capsules\",\"pods\"}\n\n# Regex patterns\nNUM = r\"(\\d{1,4}(?:[.,]\\d{1,3})?)\"\nSP = r\"[ \\-]*\"\nUNIT_PATTERN = (\n    r\"(fl\\.?\\s?oz|fluid\\s?ounce(?:s)?|floz|oz|ounce(?:s)?|g|gram(?:s)?|kg|kilogram(?:s)?|ml|milliliter(?:s)?|l|liter(?:s)?|litre(?:s)?|lb|lbs|pound(?:s)?)\"\n)\n# e.g., \"12 oz\", \"12-oz\", \"12oz\"\nRE_QTY_UNIT = re.compile(NUM + SP + UNIT_PATTERN, flags=re.IGNORECASE)\n\n# e.g., \"pack of 6\", \"6 pack\", \"6pk\", \"6 ct\", \"x6\", \"6 x 12 oz\"\nRE_PACK_PATTERNS = [\n    re.compile(r\"pack of\\s+(\\d{1,4})\", re.IGNORECASE),\n    re.compile(r\"(\\d{1,4})\\s*(?:pack|pk)\\b\", re.IGNORECASE),\n    re.compile(r\"(\\d{1,4})\\s*(?:ct|count|pcs|pieces)\\b\", re.IGNORECASE),\n    re.compile(r\"\\bx\\s*(\\d{1,4})\\b\", re.IGNORECASE),  # \"x6\"\n    re.compile(r\"(\\d{1,4})\\s*[xX]\\s*(\\d{1,4}(?:[.,]\\d{1,3})?)\\s*\"+UNIT_PATTERN, re.IGNORECASE),  # \"6 x 12 oz\"\n]\n\ndef _as_float(num_str: str) -> float:\n    if num_str is None: return np.nan\n    return float(num_str.replace(\",\", \".\"))\n\ndef extract_pack_count(text_norm: str) -> float:\n    # Try multiple patterns; choose the largest (often correct for multi-mentions)\n    vals = []\n    for pat in RE_PACK_PATTERNS:\n        for m in pat.findall(text_norm):\n            if isinstance(m, tuple):\n                # could be (pack, size, unit)\n                if len(m) >= 1:\n                    vals.append(_as_float(m[0]))\n            else:\n                vals.append(_as_float(m))\n    vals = [v for v in vals if v==v and v>0]\n    return float(max(vals)) if vals else np.nan\n\ndef _canon_unit(unit_raw: str):\n    u = unit_raw.lower().strip().replace(\"fl. oz\",\"fl oz\").replace(\"fl oz.\",\"fl oz\")\n    u = re.sub(r\"\\s+\", \" \", u)\n    if u in VOLUME_UNITS: return VOLUME_UNITS[u]\n    if u in WEIGHT_UNITS: return WEIGHT_UNITS[u]\n    # minor cleanup\n    if u.endswith(\"s\") and u[:-1] in VOLUME_UNITS: return VOLUME_UNITS[u[:-1]]\n    if u.endswith(\"s\") and u[:-1] in WEIGHT_UNITS: return WEIGHT_UNITS[u[:-1]]\n    return None\n\ndef extract_qty_units(text_norm: str):\n    \"\"\"Return list of (value_in_native, native_unit, canon_unit, canon_value) found in text.\"\"\"\n    out = []\n    for m in RE_QTY_UNIT.finditer(text_norm):\n        qty_raw, unit_raw = m.group(1), m.group(2)\n        q = _as_float(qty_raw)\n        cu = _canon_unit(unit_raw)\n        if not cu or not (q==q and q>0):\n            continue\n        canon_name, factor = cu\n        out.append((q, unit_raw.lower(), canon_name, q*factor))\n    return out\n\ndef pick_totals_and_per_item(text_norm: str):\n    \"\"\"\n    Heuristics:\n    - Find pack_count if present.\n    - Parse all qty-units; split by canon unit type (g/ml vs weight vs volume).\n    - Prefer 'per-item size * pack_count' when a clear xN pattern exists; otherwise take max total per unit-type.\n    \"\"\"\n    pack_count = extract_pack_count(text_norm)\n    pairs = extract_qty_units(text_norm)\n    if not pairs:\n        return pack_count, np.nan, np.nan, np.nan  # no totals\n\n    grams = [v for (q,u,cn,v) in pairs if cn==\"g\"]\n    mls   = [v for (q,u,cn,v) in pairs if cn==\"ml\"]\n\n    # Try to detect \"N x SIZE UNIT\": RE_PACK_PATTERNS captures some, but we’ll recompute per-item via surrounding context\n    per_item_g = np.nan\n    per_item_ml = np.nan\n\n    # Simple heuristic: if multiple same-unit sizes exist, smaller values often are 'per item'\n    if len(grams) >= 2:\n        per_item_g = min(grams)\n    if len(mls) >= 2:\n        per_item_ml = min(mls)\n\n    total_g = max(grams) if grams else np.nan\n    total_ml = max(mls) if mls else np.nan\n\n    # If we have pack_count and a plausible per-item size, prefer derived totals\n    if pack_count==pack_count and pack_count>1:\n        if per_item_g==per_item_g:\n            total_g = max(total_g, per_item_g*pack_count) if total_g==total_g else per_item_g*pack_count\n        if per_item_ml==per_item_ml:\n            total_ml = max(total_ml, per_item_ml*pack_count) if total_ml==total_ml else per_item_ml*pack_count\n\n    return pack_count, total_g, total_ml, per_item_g if per_item_g==per_item_g else per_item_ml\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:09:37.821394Z","iopub.execute_input":"2025-10-12T11:09:37.821568Z","iopub.status.idle":"2025-10-12T11:09:37.846695Z","shell.execute_reply.started":"2025-10-12T11:09:37.821546Z","shell.execute_reply":"2025-10-12T11:09:37.846176Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# ====== Cell 5R: Canonical quantity helper (add below your current Cell 5) ======\n\ndef choose_canonical_qty(total_g, total_ml, pack_count):\n    \"\"\"\n    Choose exactly ONE canonical quantity:\n      - If a volume exists, prefer ml (liquids price ~ volume).\n      - Else if a weight exists, use grams.\n      - Else if count exists, use count.\n      - Else unknown (NaN).\n    \"\"\"\n    if pd.notna(total_ml) and float(total_ml) > 0:\n        return \"ml\", float(total_ml)\n    if pd.notna(total_g) and float(total_g) > 0:\n        return \"g\", float(total_g)\n    if pd.notna(pack_count) and float(pack_count) > 0:\n        return \"count\", float(pack_count)\n    return \"unknown\", np.nan\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:09:37.847691Z","iopub.execute_input":"2025-10-12T11:09:37.847913Z","iopub.status.idle":"2025-10-12T11:09:37.869385Z","shell.execute_reply.started":"2025-10-12T11:09:37.847899Z","shell.execute_reply":"2025-10-12T11:09:37.868953Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# ====== Cell 6R: Brand lexicon from \"Item Name\" + robust mapping ======\n\nRE_ITEMNAME = re.compile(r\"item\\s*name[:\\s]+(.+)\", re.IGNORECASE)\nRE_STOP     = re.compile(r\"(?:\\b(?:value|unit|item\\s*pack\\s*quantity|ipq)\\b|[\\|\\n•]| - )\", re.IGNORECASE)\n\ndef extract_itemname_phrase(text_raw: str) -> str:\n    s = (text_raw or \"\").strip()\n    m = RE_ITEMNAME.search(s)\n    if m:\n        tail = m.group(1)\n        # cut at stop marker or punctuation divider\n        tail = RE_STOP.split(tail)[0]\n        tail = re.sub(r\"^\\s*[-:–—|•]\\s*\", \"\", tail).strip()\n        # avoid empty tails\n        if len(tail) >= 2:\n            return tail[:160]\n    # fallback: use the first \"title-like\" piece\n    return extract_title_like(s)\n\ndef brand_tokens(phrase: str):\n    # Keep tokens with letters/digits/&/'/+\n    return re.findall(r\"[A-Za-z][A-Za-z0-9&'’+-]*\", phrase)\n\ndef build_brand_lexicon(train_df: pd.DataFrame, min_count=25, lookahead=6):\n    \"\"\"\n    Build brand lexicon from first up-to-6 tokens of the Item Name phrase:\n    collect 1-3 gram prefixes sliding over the first 6 tokens; keep frequent ngrams.\n    \"\"\"\n    counts = Counter()\n    phrases = train_df[\"item_name_phrase\"].fillna(\"\").tolist()\n    for ph in phrases:\n        toks = brand_tokens(ph)\n        L = min(len(toks), lookahead)\n        for i in range(L):\n            for n in (3,2,1):\n                if i+n <= L:\n                    ng = \" \".join(toks[i:i+n])\n                    # Drop trivial ngrams\n                    if len(ng) < 2: \n                        continue\n                    # Filter extremely generic starters\n                    if ng.lower() in {\"item name\",\"brand\",\"food\",\"foods\"}:\n                        continue\n                    counts[ng] += 1\n    # Keep high-signal ngrams\n    lexicon = {ng for ng,c in counts.items() if c >= min_count}\n    return lexicon, counts\n\ndef map_brand_from_phrase(phrase: str, lexicon, counts, lookahead=6):\n    toks = brand_tokens(phrase)\n    L = min(len(toks), lookahead)\n    best = None\n    best_pos, best_len, best_cnt = 1e9, -1, -1\n    for i in range(L):\n        for n in (3,2,1):  # prefer longer n\n            if i+n <= L:\n                ng = \" \".join(toks[i:i+n])\n                if ng in lexicon:\n                    cnt = counts[ng]\n                    # choose earliest; tie-break by length then global count\n                    if (i < best_pos) or (i == best_pos and n > best_len) or (i == best_pos and n == best_len and cnt > best_cnt):\n                        best = ng\n                        best_pos, best_len, best_cnt = i, n, cnt\n    return best or \"__other__\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:09:37.870000Z","iopub.execute_input":"2025-10-12T11:09:37.870220Z","iopub.status.idle":"2025-10-12T11:09:37.892972Z","shell.execute_reply.started":"2025-10-12T11:09:37.870201Z","shell.execute_reply":"2025-10-12T11:09:37.892430Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"STOP_TOKENS = set(\"\"\"\nand & with for of the a an by in on to from new pack set combo bundle value size\n\"\"\".split())\n\ndef extract_title_like(text_raw: str) -> str:\n    \"\"\"First ~120 chars pre-colon or first sentence is often the product title.\"\"\"\n    s = (text_raw or \"\").strip()\n    # Try split by newline or period or dash; otherwise keep first 120 chars\n    for sep in [\"\\n\", \"•\", \" - \", \" | \", \" — \", \" – \", \". \"]:\n        if sep in s:\n            s = s.split(sep, 1)[0]\n            break\n    return s[:160]\n\ndef guess_brand_from_title(text_raw: str) -> str:\n    t = extract_title_like(text_raw)\n    tokens = re.split(r\"[^\\w'+&]+\", t)  # keep & and ' inside tokens\n    acc = []\n    for tok in tokens[:6]:  # first few tokens\n        if not tok: break\n        # brand tokens typically start with uppercase or are all-caps; keep numerics out\n        if tok.lower() in STOP_TOKENS: break\n        if re.match(r\"^[A-Z][\\w'+&-]*$\", tok) or re.match(r\"^[A-Z0-9&'+-]{2,}$\", tok):\n            acc.append(tok)\n        else:\n            break\n    if not acc and tokens:\n        # fallback: first token (even if lowercase), sometimes brands are lowercase stylistically\n        acc = [tokens[0]]\n    brand = \" \".join(acc).strip()\n    brand = re.sub(r\"^by\\s+\", \"\", brand, flags=re.IGNORECASE)\n    return brand if brand else \"__unknown__\"\n\nCATEGORY_KEYWORDS = {\n    \"beverage\": [\"drink\",\"beverage\",\"juice\",\"soda\",\"cola\",\"sparkling\",\"water\",\"fl oz\",\"bottle\",\"can\",\"ml\",\"l \"],\n    \"coffee_tea\": [\"coffee\",\"arabica\",\"espresso\",\"k-cup\",\"kcup\",\"k cup\",\"brew\",\"roast\",\"tea\",\"chai\",\"green tea\",\"herbal\"],\n    \"snack\": [\"chips\",\"crisps\",\"cookie\",\"cracker\",\"snack\",\"bar\",\"trail mix\",\"granola\"],\n    \"breakfast\": [\"cereal\",\"oats\",\"oatmeal\",\"pancake\",\"syrup\"],\n    \"baking\": [\"flour\",\"sugar\",\"yeast\",\"baking\",\"cocoa\",\"chocolate chip\"],\n    \"condiment\": [\"sauce\",\"ketchup\",\"mustard\",\"mayo\",\"mayonnaise\",\"dressing\",\"salsa\",\"hot sauce\",\"soy\"],\n    \"baby\": [\"baby\",\"infant\",\"toddler\",\"diaper\",\"formula\",\"pouch\"],\n    \"pet\": [\"dog\",\"cat\",\"kitten\",\"puppy\",\"pet\",\"kibble\",\"litter\"],\n    \"personal_care\": [\"shampoo\",\"conditioner\",\"soap\",\"body wash\",\"lotion\",\"deodorant\",\"toothpaste\",\"toothbrush\",\"razor\"],\n    \"household\": [\"detergent\",\"cleaner\",\"wipes\",\"trash\",\"bag\",\"paper towel\",\"tissue\"],\n    \"health\": [\"vitamin\",\"supplement\",\"capsule\",\"tablet\",\"gummy\",\"probiotic\",\"omega\",\"electrolyte\"],\n}\n\nBOOL_KEYWORDS = {\n    \"organic\": [\"organic\"],\n    \"gluten_free\": [\"gluten free\",\"gf\"],\n    \"keto\": [\"keto\"],\n    \"sugar_free\": [\"sugar free\",\"no sugar\"],\n    \"premium\": [\"premium\",\"gourmet\",\"artisan\"],\n    \"non_gmo\": [\"non-gmo\",\"nongmo\",\"non gmo\"],\n    \"kosher\": [\"kosher\"],\n    \"decaf\": [\"decaf\",\"decaffeinated\"],\n    \"instant\": [\"instant\"],\n    \"refill\": [\"refill\"],\n    \"bulk\": [\"bulk\",\"family size\",\"value size\"],\n    \"arabica\": [\"arabica\"],\n}\n\ndef coarse_category(text_norm: str) -> str:\n    for cat, kws in CATEGORY_KEYWORDS.items():\n        for kw in kws:\n            if kw in text_norm:\n                return cat\n    return \"__other__\"\n\ndef keyword_flags(text_norm: str) -> dict:\n    flags = {}\n    for name, kws in BOOL_KEYWORDS.items():\n        flags[name] = int(any(kw in text_norm for kw in kws))\n    return flags\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:09:37.893645Z","iopub.execute_input":"2025-10-12T11:09:37.893836Z","iopub.status.idle":"2025-10-12T11:09:37.914283Z","shell.execute_reply.started":"2025-10-12T11:09:37.893821Z","shell.execute_reply":"2025-10-12T11:09:37.913719Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"%%time\ndef process_df(df: pd.DataFrame, is_train: bool, top_brands=None, top_k=4000):\n    out = df.copy()\n    out[\"catalog_raw\"]  = df[\"catalog_content\"].fillna(\"\")\n    out[\"catalog_norm\"] = out[\"catalog_raw\"].map(normalize_text_for_parsing)\n\n    # Core parsing\n    packs, tots_g, tots_ml, per_item = [], [], [], []\n    lens_char, lens_word, digits_cnt = [], [], []\n\n    cats = []\n    bool_cols = list(BOOL_KEYWORDS.keys())\n    bool_matrix = {k: [] for k in bool_cols}\n\n    brands_guess = []\n\n    for raw, norm in tqdm(zip(out[\"catalog_raw\"].tolist(), out[\"catalog_norm\"].tolist()), total=len(out)):\n        p, tg, tm, pi = pick_totals_and_per_item(norm)\n        packs.append(p)\n        tots_g.append(tg)\n        tots_ml.append(tm)\n        per_item.append(pi)\n        lens_char.append(len(raw))\n        lens_word.append(len(raw.split()))\n        digits_cnt.append(len(re.findall(r\"\\d\", raw)))\n        cats.append(coarse_category(norm))\n        for k in bool_cols:\n            bool_matrix[k].append(int(any(kw in norm for kw in BOOL_KEYWORDS[k])))\n        brands_guess.append(guess_brand_from_title(raw))\n\n    out[\"pack_count\"]     = packs\n    out[\"total_g\"]        = tots_g\n    out[\"total_ml\"]       = tots_ml\n    out[\"per_item_size\"]  = per_item\n    out[\"len_chars\"]      = lens_char\n    out[\"len_words\"]      = lens_word\n    out[\"num_digits\"]     = digits_cnt\n    out[\"coarse_category\"]= cats\n    for k in bool_cols:\n        out[k] = bool_matrix[k]\n    out[\"brand_guess\"]    = brands_guess\n\n    # Choose unit type with priority: if ml present use ml; else if g present use g; else count-based\n    out[\"unit_type\"] = np.where(out[\"total_ml\"].notna(), \"ml\",\n                          np.where(out[\"total_g\"].notna(), \"g\",\n                          np.where(out[\"pack_count\"].notna(), \"count\", \"unknown\")))\n\n    # Log transforms (safe)\n    def safelog(v):\n        return np.log1p(np.clip(v.astype(float), 0, None))\n    out[\"log_total_g\"]   = safelog(out[\"total_g\"].fillna(0))\n    out[\"log_total_ml\"]  = safelog(out[\"total_ml\"].fillna(0))\n    out[\"log_pack_count\"]= safelog(out[\"pack_count\"].fillna(0))\n    out[\"log_len_words\"] = safelog(out[\"len_words\"])\n\n    # Build/Apply brand whitelist\n    if is_train:\n        freq = Counter(out[\"brand_guess\"].fillna(\"__unknown__\"))\n        top = [b for b,_ in freq.most_common(top_k)]\n        top = [t for t in top if t != \"__unknown__\"]\n        brand_whitelist = set(top)\n    else:\n        brand_whitelist = set(top_brands or [])\n\n    def map_brand(b):\n        b = (b or \"\").strip()\n        return b if b in brand_whitelist else \"__other__\"\n\n    out[\"brand_mapped\"] = out[\"brand_guess\"].map(map_brand)\n\n    meta = {\n        \"brand_whitelist\": sorted(list(brand_whitelist))\n    }\n    return out, meta\n\n# Process train\nproc_train, meta = process_df(train, is_train=True, top_k=4000)\nbrand_whitelist = meta[\"brand_whitelist\"]\n\n# Process test using the same brand whitelist\nproc_test, _ = process_df(test, is_train=False, top_brands=brand_whitelist)\n\n# Save caches\ntrain_out_path = CACHE_DIR / \"processed_train.parquet\"\ntest_out_path  = CACHE_DIR / \"processed_test.parquet\"\nmeta_path      = CACHE_DIR / \"meta.json\"\n\nproc_train.to_parquet(train_out_path, index=False)\nproc_test.to_parquet(test_out_path, index=False)\nwith open(meta_path, \"w\") as f:\n    json.dump(meta, f)\n\nprint(\"Saved:\")\nprint(\" \", train_out_path)\nprint(\" \", test_out_path)\nprint(\" \", meta_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:09:37.914902Z","iopub.execute_input":"2025-10-12T11:09:37.915120Z","iopub.status.idle":"2025-10-12T11:11:53.604291Z","shell.execute_reply.started":"2025-10-12T11:09:37.915097Z","shell.execute_reply":"2025-10-12T11:11:53.603444Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/75000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adf017d83ef4449798b5c0fd73fac484"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/75000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47ad539f779a42d1b04d892563029fc2"}},"metadata":{}},{"name":"stdout","text":"Saved:\n  /kaggle/working/cache/processed_train.parquet\n  /kaggle/working/cache/processed_test.parquet\n  /kaggle/working/cache/meta.json\nCPU times: user 2min 14s, sys: 826 ms, total: 2min 15s\nWall time: 2min 15s\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"def coverage_report(df: pd.DataFrame, name=\"train\"):\n    print(f\"=== Coverage report: {name} ===\")\n    for col in [\"pack_count\",\"total_g\",\"total_ml\",\"per_item_size\"]:\n        cov = df[col].notna().mean()*100\n        print(f\"{col:15s}: {cov:6.2f}% non-null\")\n    print(\"\\nunit_type distribution:\")\n    print(df[\"unit_type\"].value_counts(dropna=False, normalize=True).mul(100).round(2).to_string())\n    print(\"\\nbrand_mapped top 15:\")\n    print(df[\"brand_mapped\"].value_counts().head(15).to_string())\n    if \"price\" in df.columns:\n        print(\"\\nPrice summary (train):\")\n        print(df[\"price\"].describe(percentiles=[.5,.9,.99]).to_string())\n    print(\"\\nExample rows:\")\n    display(df.sample(5, random_state=SEED)[[\"sample_id\",\"brand_guess\",\"brand_mapped\",\"pack_count\",\"total_g\",\"total_ml\",\"unit_type\",\"coarse_category\",\"len_words\"]])\n\ncoverage_report(proc_train, \"train\")\ncoverage_report(proc_test, \"test\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:11:53.607111Z","iopub.execute_input":"2025-10-12T11:11:53.607324Z","iopub.status.idle":"2025-10-12T11:11:53.716006Z","shell.execute_reply.started":"2025-10-12T11:11:53.607308Z","shell.execute_reply":"2025-10-12T11:11:53.715391Z"}},"outputs":[{"name":"stdout","text":"=== Coverage report: train ===\npack_count     :  46.01% non-null\ntotal_g        :  72.35% non-null\ntotal_ml       :  19.90% non-null\nper_item_size  :  38.33% non-null\n\nunit_type distribution:\nunit_type\ng          63.66\nml         19.90\ncount       8.31\nunknown     8.13\n\nbrand_mapped top 15:\nbrand_mapped\n__other__                                   52718\nItem Name                                    1906\nItem Name Food                                962\nItem Name Frontier Co                         136\nItem Name Pride                               112\nItem Name Morton                               99\nItem Name Amazon Brand Happy Belly             87\nItem Name Harney                               86\nItem Name Snyder's                             81\nItem Name Great                                80\nItem Name Big Dot                              79\nItem Name Crystal Light                        76\nItem Name Green Mountain Coffee Roasters       74\nItem Name Marshall                             71\nItem Name Marshalls Creek Spices               69\n\nPrice summary (train):\ncount    75000.000000\nmean        23.647654\nstd         33.376932\nmin          0.130000\n50%         14.000000\n90%         52.301000\n99%        145.250300\nmax       2796.000000\n\nExample rows:\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"       sample_id                             brand_guess  \\\n26837     158784          Item Name Log Cabin Sugar Free   \n2592        4095  Item Name Raspberry Ginseng Oolong Tea   \n18359     172021      Item Name Walden Farms Honey Dijon   \n73292     268276   Item Name Vlasic Ovals Hamburger Dill   \n60127     154791  Item Name Amoretti Premium Syrup Grand   \n\n                                brand_mapped  pack_count   total_g  total_ml  \\\n26837                              __other__        12.0  680.3880   709.764   \n2592                               __other__         2.0       NaN  1000.000   \n18359                              __other__         2.0  340.1940       NaN   \n73292  Item Name Vlasic Ovals Hamburger Dill         NaN       NaN   473.176   \n60127                              __other__        12.0  720.0773       NaN   \n\n      unit_type coarse_category  len_words  \n26837        ml        beverage        101  \n2592         ml        beverage        399  \n18359         g        beverage        117  \n73292        ml        beverage         82  \n60127         g        beverage         65  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>brand_guess</th>\n      <th>brand_mapped</th>\n      <th>pack_count</th>\n      <th>total_g</th>\n      <th>total_ml</th>\n      <th>unit_type</th>\n      <th>coarse_category</th>\n      <th>len_words</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>26837</th>\n      <td>158784</td>\n      <td>Item Name Log Cabin Sugar Free</td>\n      <td>__other__</td>\n      <td>12.0</td>\n      <td>680.3880</td>\n      <td>709.764</td>\n      <td>ml</td>\n      <td>beverage</td>\n      <td>101</td>\n    </tr>\n    <tr>\n      <th>2592</th>\n      <td>4095</td>\n      <td>Item Name Raspberry Ginseng Oolong Tea</td>\n      <td>__other__</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>1000.000</td>\n      <td>ml</td>\n      <td>beverage</td>\n      <td>399</td>\n    </tr>\n    <tr>\n      <th>18359</th>\n      <td>172021</td>\n      <td>Item Name Walden Farms Honey Dijon</td>\n      <td>__other__</td>\n      <td>2.0</td>\n      <td>340.1940</td>\n      <td>NaN</td>\n      <td>g</td>\n      <td>beverage</td>\n      <td>117</td>\n    </tr>\n    <tr>\n      <th>73292</th>\n      <td>268276</td>\n      <td>Item Name Vlasic Ovals Hamburger Dill</td>\n      <td>Item Name Vlasic Ovals Hamburger Dill</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>473.176</td>\n      <td>ml</td>\n      <td>beverage</td>\n      <td>82</td>\n    </tr>\n    <tr>\n      <th>60127</th>\n      <td>154791</td>\n      <td>Item Name Amoretti Premium Syrup Grand</td>\n      <td>__other__</td>\n      <td>12.0</td>\n      <td>720.0773</td>\n      <td>NaN</td>\n      <td>g</td>\n      <td>beverage</td>\n      <td>65</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"=== Coverage report: test ===\npack_count     :  45.83% non-null\ntotal_g        :  72.41% non-null\ntotal_ml       :  19.94% non-null\nper_item_size  :  38.72% non-null\n\nunit_type distribution:\nunit_type\ng          63.74\nml         19.94\ncount       8.45\nunknown     7.88\n\nbrand_mapped top 15:\nbrand_mapped\n__other__                                   58492\nItem Name                                    1916\nItem Name Food                                964\nItem Name Frontier Co                         123\nItem Name Harney                               93\nItem Name Pride                                91\nItem Name Morton                               85\nItem Name Amazon Brand Happy Belly             84\nItem Name Great                                79\nItem Name Marshall                             75\nItem Name Marshalls Creek Spices               67\nItem Name Big Dot                              67\nItem Name Green Mountain Coffee Roasters       66\nItem Name Chicken                              61\nItem Name Crystal Light                        59\n\nExample rows:\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"       sample_id                            brand_guess brand_mapped  \\\n26837     217392  Item Name Gift Basket Village Gourmet    __other__   \n2592      209156        Item Name NPG Dried Lotus Seeds    __other__   \n18359     262333    Item Name Annies Homegrown Macaroni    __other__   \n73292     295979  Item Name Bear Creek Country Kitchens    __other__   \n60127      50604    Item Name Japanese Kelp Kombu Umami    __other__   \n\n       pack_count    total_g  total_ml unit_type coarse_category  len_words  \n26837         NaN  198.44650       NaN         g        beverage        545  \n2592          NaN  454.00000       NaN         g        beverage        187  \n18359         NaN  170.09700       NaN         g        beverage         54  \n73292         NaN  286.32995       NaN         g        beverage        112  \n60127        10.0  396.89300    591.47        ml        beverage        195  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>brand_guess</th>\n      <th>brand_mapped</th>\n      <th>pack_count</th>\n      <th>total_g</th>\n      <th>total_ml</th>\n      <th>unit_type</th>\n      <th>coarse_category</th>\n      <th>len_words</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>26837</th>\n      <td>217392</td>\n      <td>Item Name Gift Basket Village Gourmet</td>\n      <td>__other__</td>\n      <td>NaN</td>\n      <td>198.44650</td>\n      <td>NaN</td>\n      <td>g</td>\n      <td>beverage</td>\n      <td>545</td>\n    </tr>\n    <tr>\n      <th>2592</th>\n      <td>209156</td>\n      <td>Item Name NPG Dried Lotus Seeds</td>\n      <td>__other__</td>\n      <td>NaN</td>\n      <td>454.00000</td>\n      <td>NaN</td>\n      <td>g</td>\n      <td>beverage</td>\n      <td>187</td>\n    </tr>\n    <tr>\n      <th>18359</th>\n      <td>262333</td>\n      <td>Item Name Annies Homegrown Macaroni</td>\n      <td>__other__</td>\n      <td>NaN</td>\n      <td>170.09700</td>\n      <td>NaN</td>\n      <td>g</td>\n      <td>beverage</td>\n      <td>54</td>\n    </tr>\n    <tr>\n      <th>73292</th>\n      <td>295979</td>\n      <td>Item Name Bear Creek Country Kitchens</td>\n      <td>__other__</td>\n      <td>NaN</td>\n      <td>286.32995</td>\n      <td>NaN</td>\n      <td>g</td>\n      <td>beverage</td>\n      <td>112</td>\n    </tr>\n    <tr>\n      <th>60127</th>\n      <td>50604</td>\n      <td>Item Name Japanese Kelp Kombu Umami</td>\n      <td>__other__</td>\n      <td>10.0</td>\n      <td>396.89300</td>\n      <td>591.47</td>\n      <td>ml</td>\n      <td>beverage</td>\n      <td>195</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"keep_cols_common = [\n    \"sample_id\",\"catalog_raw\",\"catalog_norm\",\n    \"brand_guess\",\"brand_mapped\",\"coarse_category\",\n    \"pack_count\",\"total_g\",\"total_ml\",\"per_item_size\",\n    \"len_chars\",\"len_words\",\"num_digits\",\n    \"log_total_g\",\"log_total_ml\",\"log_pack_count\",\"log_len_words\",\n    \"unit_type\",\n] + list(BOOL_KEYWORDS.keys())\n\ntrain_feats = proc_train[keep_cols_common + [\"price\"]].copy()\ntest_feats  = proc_test[keep_cols_common].copy()\n\n# Re-save slim versions\ntrain_feats_path = CACHE_DIR / \"train_feats.parquet\"\ntest_feats_path  = CACHE_DIR / \"test_feats.parquet\"\ntrain_feats.to_parquet(train_feats_path, index=False)\ntest_feats.to_parquet(test_feats_path, index=False)\n\nprint(\"Slim features saved:\")\nprint(\" \", train_feats_path)\nprint(\" \", test_feats_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:11:53.716647Z","iopub.execute_input":"2025-10-12T11:11:53.716914Z","iopub.status.idle":"2025-10-12T11:11:55.646057Z","shell.execute_reply.started":"2025-10-12T11:11:53.716889Z","shell.execute_reply":"2025-10-12T11:11:55.645289Z"}},"outputs":[{"name":"stdout","text":"Slim features saved:\n  /kaggle/working/cache/train_feats.parquet\n  /kaggle/working/cache/test_feats.parquet\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"%%time\ndef process_df_v2(df: pd.DataFrame, is_train: bool, brand_lexicon=None, brand_counts=None, top_k=4000):\n    out = df.copy()\n    out[\"catalog_raw\"]  = df[\"catalog_content\"].fillna(\"\")\n    out[\"catalog_norm\"] = out[\"catalog_raw\"].map(normalize_text_for_parsing)\n\n    # ----- core parsing (re-use existing pick_totals_and_per_item) -----\n    packs, tots_g, tots_ml, per_item = [], [], [], []\n    lens_char, lens_word, digits_cnt = [], [], []\n\n    cats = []\n    bool_cols = list(BOOL_KEYWORDS.keys())\n    bool_matrix = {k: [] for k in bool_cols}\n\n    # NEW: item name phrase\n    itemname_phrases = []\n\n    for raw, norm in tqdm(zip(out[\"catalog_raw\"].tolist(), out[\"catalog_norm\"].tolist()), total=len(out)):\n        p, tg, tm, pi = pick_totals_and_per_item(norm)\n        packs.append(p)\n        tots_g.append(tg)\n        tots_ml.append(tm)\n        per_item.append(pi)\n        lens_char.append(len(raw))\n        lens_word.append(len(raw.split()))\n        digits_cnt.append(len(re.findall(r\"\\d\", raw)))\n        cats.append(coarse_category(norm))\n        for k in bool_cols:\n            bool_matrix[k].append(int(any(kw in norm for kw in BOOL_KEYWORDS[k])))\n        itemname_phrases.append(extract_itemname_phrase(raw))\n\n    out[\"pack_count\"]     = packs\n    out[\"total_g\"]        = tots_g\n    out[\"total_ml\"]       = tots_ml\n    out[\"per_item_size\"]  = per_item\n    out[\"len_chars\"]      = lens_char\n    out[\"len_words\"]      = lens_word\n    out[\"num_digits\"]     = digits_cnt\n    out[\"coarse_category\"]= cats\n    for k in bool_cols:\n        out[k] = bool_matrix[k]\n    out[\"item_name_phrase\"] = itemname_phrases\n\n    # ----- canonical quantity -----\n    qty_types, qty_vals = [], []\n    for g, ml, pk in zip(out[\"total_g\"], out[\"total_ml\"], out[\"pack_count\"]):\n        t, v = choose_canonical_qty(g, ml, pk)\n        qty_types.append(t); qty_vals.append(v)\n    out[\"qty_type\"]        = qty_types\n    out[\"total_qty_std\"]   = qty_vals\n\n    def safelog(v):\n        return np.log1p(np.clip(pd.Series(v, dtype=\"float64\"), 0, None))\n    out[\"log_total_qty_std\"] = safelog(out[\"total_qty_std\"])\n    out[\"log_len_words\"]     = safelog(out[\"len_words\"])\n    out[\"log_pack_count\"]    = safelog(out[\"pack_count\"].fillna(0))\n\n    # ----- brand lexicon build/map -----\n    if is_train:\n        # build lexicon from train phrases\n        tmp = out[[\"item_name_phrase\"]].copy()\n        brand_lexicon, brand_counts = build_brand_lexicon(tmp, min_count=25, lookahead=6)\n\n    # map brand for both train/test using the train-built lexicon\n    mapped = [map_brand_from_phrase(ph, brand_lexicon, brand_counts) for ph in out[\"item_name_phrase\"]]\n    out[\"brand_mapped_v2\"] = mapped\n\n    meta = {\n        \"brand_lexicon\": sorted(list(brand_lexicon)),\n        \"brand_counts\": {k:int(v) for k,v in list(brand_counts.items())[:100000]}  # cap for JSON size\n    }\n    return out, meta\n\n# ---- Run on train to build lexicon ----\nproc_train_v2, meta_v2 = process_df_v2(train, is_train=True)\nlexicon = set(meta_v2[\"brand_lexicon\"])\ncounts_map = Counter(meta_v2[\"brand_counts\"])\n\n# ---- Run on test using the same lexicon ----\nproc_test_v2, _ = process_df_v2(test, is_train=False, brand_lexicon=lexicon, brand_counts=counts_map)\n\n# ---- Save v2 caches ----\nV2_DIR = CACHE_DIR / \"v2\"\nV2_DIR.mkdir(parents=True, exist_ok=True)\n\nproc_train_v2.to_parquet(V2_DIR/\"processed_train_v2.parquet\", index=False)\nproc_test_v2.to_parquet(V2_DIR/\"processed_test_v2.parquet\", index=False)\n\n# Slim feature sets for modeling (v2)\nkeep_cols_common_v2 = [\n    \"sample_id\",\"catalog_raw\",\"catalog_norm\",\"item_name_phrase\",\n    \"brand_mapped_v2\",\"coarse_category\",\n    \"pack_count\",\"total_g\",\"total_ml\",\"total_qty_std\",\"qty_type\",\"per_item_size\",\n    \"len_chars\",\"len_words\",\"num_digits\",\n    \"log_total_qty_std\",\"log_len_words\",\"log_pack_count\",\n] + list(BOOL_KEYWORDS.keys())\n\ntrain_feats_v2 = proc_train_v2[keep_cols_common_v2 + [\"price\"]].copy()\ntest_feats_v2  = proc_test_v2[keep_cols_common_v2].copy()\n\ntrain_feats_v2.to_parquet(V2_DIR/\"train_feats_v2.parquet\", index=False)\ntest_feats_v2.to_parquet(V2_DIR/\"test_feats_v2.parquet\", index=False)\n\nwith open(V2_DIR/\"meta_v2.json\", \"w\") as f:\n    json.dump({\"n_lexicon\": len(lexicon)}, f)\n\nprint(\"Saved v2:\")\nprint(\" \", V2_DIR/\"processed_train_v2.parquet\")\nprint(\" \", V2_DIR/\"processed_test_v2.parquet\")\nprint(\" \", V2_DIR/\"train_feats_v2.parquet\")\nprint(\" \", V2_DIR/\"test_feats_v2.parquet\")\n\n\ndef coverage_report_v2(df: pd.DataFrame, name=\"train_v2\"):\n    print(f\"=== Coverage report: {name} ===\")\n    cov = df[\"total_qty_std\"].notna().mean()*100\n    print(f\"total_qty_std : {cov:6.2f}% non-null\")\n    print(\"qty_type distribution (%):\")\n    print(df[\"qty_type\"].value_counts(dropna=False, normalize=True).mul(100).round(2).to_string())\n    print(\"\\nbrand_mapped_v2 top 20:\")\n    print(df[\"brand_mapped_v2\"].value_counts().head(20).to_string())\n    if \"price\" in df.columns:\n        print(\"\\nPrice summary:\")\n        print(df[\"price\"].describe(percentiles=[.5,.9,.99]).to_string())\n\ntrain_v2 = pd.read_parquet(V2_DIR/\"processed_train_v2.parquet\")\ntest_v2  = pd.read_parquet(V2_DIR/\"processed_test_v2.parquet\")\n\ncoverage_report_v2(train_v2, \"train_v2\")\ncoverage_report_v2(test_v2,  \"test_v2\")\n\nprint(\"\\nSample rows:\")\nprint(train_v2.sample(5, random_state=SEED)[[\"sample_id\",\"item_name_phrase\",\"brand_mapped_v2\",\"qty_type\",\"total_qty_std\",\"pack_count\"]].to_string(index=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:11:55.646909Z","iopub.execute_input":"2025-10-12T11:11:55.647207Z","iopub.status.idle":"2025-10-12T11:14:15.983335Z","shell.execute_reply.started":"2025-10-12T11:11:55.647184Z","shell.execute_reply":"2025-10-12T11:14:15.982413Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/75000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69fbbe68bb7442938b2cc038146c1c7b"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/75000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a84876ab63bc45faba08373c756a61ae"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal\n  return op(a, b)\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Saved v2:\n  /kaggle/working/cache/v2/processed_train_v2.parquet\n  /kaggle/working/cache/v2/processed_test_v2.parquet\n  /kaggle/working/cache/v2/train_feats_v2.parquet\n  /kaggle/working/cache/v2/test_feats_v2.parquet\n=== Coverage report: train_v2 ===\ntotal_qty_std :  91.87% non-null\nqty_type distribution (%):\nqty_type\ng          63.66\nml         19.90\ncount       8.31\nunknown     8.13\n\nbrand_mapped_v2 top 20:\nbrand_mapped_v2\n__other__         1213\nOrganic           1013\nFood to Live       945\nGourmet            389\nOriginal           373\nRani               358\nMcCormick          339\nThe                326\nBetty Crocker      315\nPremium            302\nBadia              285\nBob's Red Mill     272\nGoya               261\nCoffee             258\nStarbucks          247\nKraft              247\nAmoretti           235\nNatural            230\nRed                229\nChocolate          216\n\nPrice summary:\ncount    75000.000000\nmean        23.647654\nstd         33.376932\nmin          0.130000\n50%         14.000000\n90%         52.301000\n99%        145.250300\nmax       2796.000000\n=== Coverage report: test_v2 ===\ntotal_qty_std :  92.12% non-null\nqty_type distribution (%):\nqty_type\ng          63.74\nml         19.94\ncount       8.45\nunknown     7.88\n\nbrand_mapped_v2 top 20:\nbrand_mapped_v2\n__other__         1228\nOrganic           1052\nFood to Live       947\nGourmet            398\nOriginal           375\nMcCormick          370\nRani               351\nThe                331\nPremium            314\nBetty Crocker      307\nBob's Red Mill     280\nBadia              275\nNatural            260\nAmoretti           258\nGoya               255\nKraft              254\nRed                244\nCoffee             237\nCandy              232\nStarbucks          232\n\nSample rows:\n sample_id                                                  item_name_phrase  brand_mapped_v2 qty_type  total_qty_std  pack_count\n    158784                 Log Cabin Sugar Free Syrup, 24 FL OZ (Pack of 12) Sugar Free Syrup       ml       709.7640        12.0\n      4095           Raspberry Ginseng Oolong Tea (50 tea bags, ZIN: 543034)        Raspberry       ml      1000.0000         2.0\n    172021                                 Walden Farms Honey Dijon Dressing     Walden Farms        g       340.1940         2.0\n    268276 Vlasic Ovals Hamburger Dill Pickle Chips, Keto Friendly, 16 FL OZ           Vlasic       ml       473.1760         NaN\n    154791     Amoretti Premium Syrup, Grand Orange, 25.4 Ounce (Pack of 12) Amoretti Premium        g       720.0773        12.0\nCPU times: user 2min 19s, sys: 2.46 s, total: 2min 21s\nWall time: 2min 20s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"%%capture\n!pip install -U transformers accelerate sentencepiece safetensors","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:14:15.984258Z","iopub.execute_input":"2025-10-12T11:14:15.984555Z","iopub.status.idle":"2025-10-12T11:15:55.289738Z","shell.execute_reply.started":"2025-10-12T11:14:15.984531Z","shell.execute_reply":"2025-10-12T11:15:55.288311Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntorch.backends.cuda.matmul.allow_tf32 = True\n\nMODEL_NAME = \"google/flan-t5-small\"  # ~80M params\ntok = AutoTokenizer.from_pretrained(MODEL_NAME)\nmdl = AutoModelForSeq2SeqLM.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.float16 if device==\"cuda\" else torch.float32\n).to(device)\n\nprint(\"Device:\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:15:55.291142Z","iopub.execute_input":"2025-10-12T11:15:55.291546Z","iopub.status.idle":"2025-10-12T11:16:31.271862Z","shell.execute_reply.started":"2025-10-12T11:15:55.291514Z","shell.execute_reply":"2025-10-12T11:16:31.271047Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41ab73637fb0442d853405c5b957df99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48929660dc7e4f5a9265459f1fd926d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d67409afed74985a3185330a7350440"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cdec52626974fbe8fdaf1f82c2b8240"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a2f5472904d43729fbae3d647d76c74"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n2025-10-12 11:16:12.208258: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760267772.579254      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760267772.692339      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"380dcedf3ad646b0b06c14b913e11413"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad3bccd2562b47438e296078e7b30954"}},"metadata":{}},{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"import json, re\nfrom typing import List, Tuple, Optional\n\n# Words that are *not* brands by themselves\nGENERIC_SINGLE_WORDS = {\n    \"Organic\",\"Gourmet\",\"Original\",\"Premium\",\"Natural\",\"The\",\n    \"Chocolate\",\"Coffee\",\"Candy\",\"Red\",\"Blue\",\"Green\",\"Classic\",\"Fresh\",\"New\",\"Value\"\n}\n# Phrases to trim if LLM returns full phrases like \"Amazon Brand Happy Belly\"\nTRIM_PATTERNS = [\n    (re.compile(r\"amazon brand\\s+\", re.I), \"\"),   # Keep just \"Happy Belly\"\n    (re.compile(r\"\\bbrand\\b\", re.I), \"\"),         # trailing \"Brand\"\n]\n\ndef clean_brand_text(b: str) -> str:\n    if not b: return \"\"\n    s = b.strip()\n    # Trim known wrappers\n    for pat, repl in TRIM_PATTERNS:\n        s = pat.sub(repl, s).strip()\n    # Normalize spaces/punctuation\n    s = re.sub(r\"\\s{2,}\", \" \", s)\n    # Keep reasonable chars\n    s = re.sub(r\"[^A-Za-z0-9&'’\\-\\.\\s]+\", \"\", s).strip()\n    # Special fixes\n    s = s.replace(\"Mccormick\", \"McCormick\")\n    s = s.replace(\"Bobs Red Mill\", \"Bob's Red Mill\")\n    # Uppercase sequences are fine; otherwise title-case lightly (preserve Mc prefixes)\n    def smart_tc(word):\n        if re.fullmatch(r\"[A-Z0-9&'’\\-]+\", word):  # all-caps tokens\n            return word\n        if word.lower().startswith(\"mc\") and len(word)>=3:\n            return \"Mc\" + word[2:].capitalize()\n        return word.capitalize()\n    s = \" \".join(smart_tc(w) for w in s.split())\n    return s\n\ndef is_valid_brand(b: str) -> bool:\n    if not b: return False\n    # Reject single generic words\n    if b in GENERIC_SINGLE_WORDS: return False\n    # Length and alphabetic check\n    if len(re.sub(r\"[^A-Za-z]+\",\"\", b)) < 2: return False\n    # Heuristic: at least one letter, not pure category words\n    bad = {\"Sugar Free\",\"Original\",\"Gourmet\",\"Organic\",\"Premium\",\"The\",\"Candy\",\"Chocolate\",\"Coffee\",\"Natural\"}\n    if b in bad: return False\n    return True\n\nFEWSHOTS = [\n    # Keep these short; T5 small learns JSON structure with a couple of examples\n    {\n      \"text\": \"Log Cabin Sugar Free Syrup, 24 FL OZ (Pack of 12)\",\n      \"json\": {\"brand\": \"Log Cabin\", \"pack_count\": 12}\n    },\n    {\n      \"text\": \"Amazon Brand - Happy Belly Mixed Nuts, 16 oz, 2 Pack\",\n      \"json\": {\"brand\": \"Happy Belly\", \"pack_count\": 2}\n    },\n    {\n      \"text\": \"Starbucks Pike Place Roast K-Cup Coffee Pods, 44 ct\",\n      \"json\": {\"brand\": \"Starbucks\", \"pack_count\": 44}\n    },\n]\n\nINSTR = (\n  \"Extract the MANUFACTURER BRAND and PACK_COUNT (integer if clearly present) from the product text.\\n\"\n  \"Rules: brand must be a proper name (not adjectives like Organic/Gourmet/Premium). \"\n  \"If text says 'Amazon Brand - Happy Belly', return 'Happy Belly' as the brand. \"\n  \"Return JSON only with keys: brand (string or null) and pack_count (integer or null).\"\n)\n\ndef make_prompt(text: str) -> str:\n    shots = \"\"\n    for ex in FEWSHOTS:\n        shots += f\"Text: {ex['text']}\\nJSON: {json.dumps(ex['json'])}\\n\\n\"\n    return f\"{INSTR}\\n\\n{shots}Text: {text}\\nJSON:\"\n\ndef parse_json(s: str) -> dict:\n    m = re.search(r\"\\{.*\\}\", s, flags=re.S)\n    if not m:\n        return {}\n    try:\n        return json.loads(m.group(0))\n    except Exception:\n        # crude fixes for single quotes or trailing commas\n        t = m.group(0).replace(\"'\", '\"')\n        t = re.sub(r\",\\s*}\", \"}\", t)\n        try:\n            return json.loads(t)\n        except Exception:\n            return {}\n\n@torch.no_grad()\ndef llm_extract_batch(texts: List[str], max_new_tokens=48) -> Tuple[List[Optional[str]], List[Optional[int]]]:\n    prompts = [make_prompt(t[:600]) for t in texts]  # truncate long texts\n    enc = tok(prompts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n    gen = mdl.generate(**enc, do_sample=False, num_beams=1, max_new_tokens=max_new_tokens)\n    outs = tok.batch_decode(gen, skip_special_tokens=True)\n    brands, packs = [], []\n    for o in outs:\n        js = parse_json(o)\n        b = js.get(\"brand\") if isinstance(js, dict) else None\n        p = js.get(\"pack_count\") if isinstance(js, dict) else None\n        b = clean_brand_text(b) if b else \"\"\n        if not is_valid_brand(b):\n            b = \"\"\n        try:\n            p = int(p) if p is not None else None\n            if p is not None and (p <= 0 or p > 10000):\n                p = None\n        except Exception:\n            p = None\n        brands.append(b if b else None)\n        packs.append(p)\n    return brands, packs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:16:31.272671Z","iopub.execute_input":"2025-10-12T11:16:31.273173Z","iopub.status.idle":"2025-10-12T11:16:31.287873Z","shell.execute_reply.started":"2025-10-12T11:16:31.273153Z","shell.execute_reply":"2025-10-12T11:16:31.287119Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# Load v2 frames created earlier\nV2_DIR   = CACHE_DIR / \"v2\"\ntrain_v2 = pd.read_parquet(V2_DIR/\"processed_train_v2.parquet\")\ntest_v2  = pd.read_parquet(V2_DIR/\"processed_test_v2.parquet\")\n\n# Rows that look suspicious: __other__ or single generic words\nSUSPECT = GENERIC_SINGLE_WORDS | {\"__other__\"}\nmask_train = (train_v2[\"brand_mapped_v2\"].isin(SUSPECT))\nmask_test  = (test_v2[\"brand_mapped_v2\"].isin(SUSPECT))\n\nprint(\"Suspicious in train:\", int(mask_train.sum()), \" / \", len(train_v2))\nprint(\"Suspicious in test :\", int(mask_test.sum()),  \" / \", len(test_v2))\n\nBATCH = 128  # safe on T4; reduce to 64 if you OOM\n\ndef run_llm_on_frame(df, mask):\n    idx = df.index[mask].tolist()\n    texts = df.loc[idx, \"catalog_raw\"].tolist()\n    brands, packs = [], []\n    for i in range(0, len(texts), BATCH):\n        br, pk = llm_extract_batch(texts[i:i+BATCH])\n        brands.extend(br); packs.extend(pk)\n    out = pd.DataFrame({\n        \"idx\": idx,\n        \"brand_llm\": brands,\n        \"pack_count_llm\": packs\n    })\n    return out\n\ntrain_llm = run_llm_on_frame(train_v2, mask_train)\ntest_llm  = run_llm_on_frame(test_v2,  mask_test)\n\n# Merge back\ntrain_v2 = train_v2.merge(train_llm, how=\"left\", left_index=True, right_on=\"idx\").drop(columns=[\"idx\"])\ntest_v2  = test_v2.merge(test_llm,  how=\"left\", left_index=True, right_on=\"idx\").drop(columns=[\"idx\"])\n\n# Fill NaNs for non-suspicious rows\ntrain_v2[\"brand_llm\"] = train_v2[\"brand_llm\"].fillna(\"\")\ntest_v2[\"brand_llm\"]  = test_v2[\"brand_llm\"].fillna(\"\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:16:31.288721Z","iopub.execute_input":"2025-10-12T11:16:31.289008Z","iopub.status.idle":"2025-10-12T11:18:34.987194Z","shell.execute_reply.started":"2025-10-12T11:16:31.288982Z","shell.execute_reply":"2025-10-12T11:18:34.986158Z"}},"outputs":[{"name":"stdout","text":"Suspicious in train: 5312  /  75000\nSuspicious in test : 5460  /  75000\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"def choose_brand_final(row):\n    # Prefer LLM brand if present and valid; else keep v2 rule\n    b_llm = row.get(\"brand_llm\") or \"\"\n    if b_llm and is_valid_brand(b_llm):\n        return clean_brand_text(b_llm)\n    return row[\"brand_mapped_v2\"]\n\ndef pick_pack_final(row):\n    # prefer parsed pack if original missing or zero\n    pk0 = row[\"pack_count\"]\n    pkl = row.get(\"pack_count_llm\", None)\n    if pd.isna(pk0) or (isinstance(pk0, (int,float)) and pk0 <= 0):\n        return float(pkl) if pkl is not None else pk0\n    return pk0\n\nfor df in (train_v2, test_v2):\n    df[\"brand_final\"] = df.apply(choose_brand_final, axis=1)\n    df[\"pack_count_final\"] = df.apply(pick_pack_final, axis=1)\n\n# Update canonical quantity for rows that had unknown and now have pack_count\ndef recompute_qty_cols(df):\n    qty_type_final = []\n    total_qty_final = []\n    for qt, tq, pk in zip(df[\"qty_type\"], df[\"total_qty_std\"], df[\"pack_count_final\"]):\n        if qt != \"unknown\":\n            qty_type_final.append(qt)\n            total_qty_final.append(tq)\n        else:\n            if pd.notna(pk) and float(pk) > 0:\n                qty_type_final.append(\"count\")\n                total_qty_final.append(float(pk))\n            else:\n                qty_type_final.append(qt)\n                total_qty_final.append(np.nan)\n    df[\"qty_type_final\"] = qty_type_final\n    df[\"total_qty_std_final\"] = total_qty_final\n    df[\"log_total_qty_std_final\"] = np.log1p(pd.Series(total_qty_final, dtype=\"float64\").fillna(0.0))\n\nrecompute_qty_cols(train_v2)\nrecompute_qty_cols(test_v2)\n\n# Quick peek\nprint(\"brand_mapped_v2  -> brand_final (train) top 20\")\nprint(train_v2[\"brand_final\"].value_counts().head(20).to_string())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:18:34.988155Z","iopub.execute_input":"2025-10-12T11:18:34.988430Z","iopub.status.idle":"2025-10-12T11:18:37.197772Z","shell.execute_reply.started":"2025-10-12T11:18:34.988403Z","shell.execute_reply":"2025-10-12T11:18:37.196937Z"}},"outputs":[{"name":"stdout","text":"brand_mapped_v2  -> brand_final (train) top 20\nbrand_final\n__other__         1213\nOrganic           1013\nFood to Live       945\nGourmet            389\nOriginal           373\nRani               358\nMcCormick          339\nThe                326\nBetty Crocker      315\nPremium            302\nBadia              285\nBob's Red Mill     272\nGoya               261\nCoffee             258\nStarbucks          247\nKraft              247\nAmoretti           235\nNatural            230\nRed                229\nChocolate          216\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"V2LLM_DIR = CACHE_DIR / \"v2_llm\"\nV2LLM_DIR.mkdir(parents=True, exist_ok=True)\n\ntrain_v2.to_parquet(V2LLM_DIR/\"processed_train_v2_llm.parquet\", index=False)\ntest_v2.to_parquet(V2LLM_DIR/\"processed_test_v2_llm.parquet\", index=False)\n\n# Slim feature views (we will use brand_final & qty_type_final going forward)\nkeep_cols_common_v2llm = [\n    \"sample_id\",\"catalog_raw\",\"catalog_norm\",\"item_name_phrase\",\n    \"brand_final\",\"coarse_category\",\n    \"pack_count_final\",\"total_g\",\"total_ml\",\"per_item_size\",\n    \"qty_type_final\",\"total_qty_std_final\",\"log_total_qty_std_final\",\n    \"len_chars\",\"len_words\",\"num_digits\",\"log_len_words\",\"log_pack_count\",\n] + list(BOOL_KEYWORDS.keys())\n\ntrain_feats_v2llm = train_v2[keep_cols_common_v2llm + [\"price\"]].copy()\ntest_feats_v2llm  = test_v2[keep_cols_common_v2llm].copy()\n\ntrain_feats_v2llm.to_parquet(V2LLM_DIR/\"train_feats_v2_llm.parquet\", index=False)\ntest_feats_v2llm.to_parquet(V2LLM_DIR/\"test_feats_v2_llm.parquet\", index=False)\n\nprint(\"Saved v2_llm features:\")\nprint(\" \", V2LLM_DIR/\"train_feats_v2_llm.parquet\")\nprint(\" \", V2LLM_DIR/\"test_feats_v2_llm.parquet\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:18:37.198711Z","iopub.execute_input":"2025-10-12T11:18:37.199385Z","iopub.status.idle":"2025-10-12T11:18:41.881584Z","shell.execute_reply.started":"2025-10-12T11:18:37.199356Z","shell.execute_reply":"2025-10-12T11:18:41.880927Z"}},"outputs":[{"name":"stdout","text":"Saved v2_llm features:\n  /kaggle/working/cache/v2_llm/train_feats_v2_llm.parquet\n  /kaggle/working/cache/v2_llm/test_feats_v2_llm.parquet\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"def sanity_after_llm(df, name=\"train_v2_llm\"):\n    print(f\"=== {name} ===\")\n    print(\"brand_final top 20:\")\n    print(df[\"brand_final\"].value_counts().head(20).to_string())\n    print(\"\\nqty_type_final distribution (%):\")\n    print(df[\"qty_type_final\"].value_counts(dropna=False, normalize=True).mul(100).round(2).to_string())\n    if \"price\" in df.columns:\n        print(\"\\nPrice summary:\")\n        print(df[\"price\"].describe(percentiles=[.5,.9,.99]).to_string())\n    # coverage for final qty\n    cov = df[\"total_qty_std_final\"].notna().mean()*100\n    print(f\"\\nfinal total_qty_std coverage: {cov:0.2f}%\")\n\ntrain_v2_llm = pd.read_parquet(V2LLM_DIR/\"processed_train_v2_llm.parquet\")\ntest_v2_llm  = pd.read_parquet(V2LLM_DIR/\"processed_test_v2_llm.parquet\")\n\nsanity_after_llm(train_v2_llm, \"train_v2_llm\")\nsanity_after_llm(test_v2_llm,  \"test_v2_llm\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:18:41.882294Z","iopub.execute_input":"2025-10-12T11:18:41.882591Z","iopub.status.idle":"2025-10-12T11:18:43.563691Z","shell.execute_reply.started":"2025-10-12T11:18:41.882572Z","shell.execute_reply":"2025-10-12T11:18:43.562969Z"}},"outputs":[{"name":"stdout","text":"=== train_v2_llm ===\nbrand_final top 20:\nbrand_final\n__other__         1213\nOrganic           1013\nFood to Live       945\nGourmet            389\nOriginal           373\nRani               358\nMcCormick          339\nThe                326\nBetty Crocker      315\nPremium            302\nBadia              285\nBob's Red Mill     272\nGoya               261\nCoffee             258\nStarbucks          247\nKraft              247\nAmoretti           235\nNatural            230\nRed                229\nChocolate          216\n\nqty_type_final distribution (%):\nqty_type_final\ng          63.66\nml         19.90\ncount       8.31\nunknown     8.13\n\nPrice summary:\ncount    75000.000000\nmean        23.647654\nstd         33.376932\nmin          0.130000\n50%         14.000000\n90%         52.301000\n99%        145.250300\nmax       2796.000000\n\nfinal total_qty_std coverage: 91.87%\n=== test_v2_llm ===\nbrand_final top 20:\nbrand_final\n__other__         1228\nOrganic           1052\nFood to Live       947\nGourmet            398\nOriginal           375\nMcCormick          370\nRani               351\nThe                331\nPremium            314\nBetty Crocker      307\nBob's Red Mill     280\nBadia              275\nNatural            260\nAmoretti           258\nGoya               255\nKraft              254\nRed                244\nCoffee             237\nCandy              232\nStarbucks          232\n\nqty_type_final distribution (%):\nqty_type_final\ng          63.74\nml         19.94\ncount       8.45\nunknown     7.88\n\nfinal total_qty_std coverage: 92.12%\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"print(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:18:43.564477Z","iopub.execute_input":"2025-10-12T11:18:43.564736Z","iopub.status.idle":"2025-10-12T11:18:43.569155Z","shell.execute_reply.started":"2025-10-12T11:18:43.564713Z","shell.execute_reply":"2025-10-12T11:18:43.568385Z"}},"outputs":[{"name":"stdout","text":"5\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"import os, gc, re, json, hashlib\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_absolute_error\nfrom scipy import sparse\n\n# Our SMAPE from earlier\ndef smape(y_true, y_pred, eps=1e-9):\n    num = np.abs(y_pred - y_true)\n    den = (np.abs(y_true) + np.abs(y_pred) + eps)/2.0\n    return 100.0 * np.mean(num/den)\n\nSEED = 42\nnp.random.seed(SEED)\n\nWORK_DIR  = Path(\"/kaggle/working\")\nCACHE_DIR = WORK_DIR / \"cache\"\nV2LLM_DIR = CACHE_DIR / \"v2_llm\"\n\ntrain = pd.read_parquet(V2LLM_DIR / \"train_feats_v2_llm.parquet\")\ntest  = pd.read_parquet(V2LLM_DIR / \"test_feats_v2_llm.parquet\")\n\nprint(train.shape, test.shape)\ntrain.head(2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:18:43.570003Z","iopub.execute_input":"2025-10-12T11:18:43.570594Z","iopub.status.idle":"2025-10-12T11:18:45.504935Z","shell.execute_reply.started":"2025-10-12T11:18:43.570576Z","shell.execute_reply":"2025-10-12T11:18:45.504181Z"}},"outputs":[{"name":"stdout","text":"(75000, 31) (75000, 30)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"   sample_id                                        catalog_raw  \\\n0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n1     198967  Item Name: Salerno Cookies, The Original Butte...   \n\n                                        catalog_norm  \\\n0  item name: la victoria green taco sauce mild, ...   \n1  item name: salerno cookies, the original butte...   \n\n                                    item_name_phrase  brand_final  \\\n0  La Victoria Green Taco Sauce Mild, 12 Ounce (P...  La Victoria   \n1  Salerno Cookies, The Original Butter Cookies, ...      Cookies   \n\n  coarse_category  pack_count_final  total_g  total_ml  per_item_size  ...  \\\n0        beverage               6.0  340.194       NaN            NaN  ...   \n1        beverage               4.0  226.796       NaN            NaN  ...   \n\n  sugar_free  premium  non_gmo  kosher  decaf  instant  refill  bulk  arabica  \\\n0          0        0        0       0      0        0       0     0        0   \n1          0        0        0       0      0        0       0     0        0   \n\n   price  \n0   4.89  \n1  13.12  \n\n[2 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>catalog_raw</th>\n      <th>catalog_norm</th>\n      <th>item_name_phrase</th>\n      <th>brand_final</th>\n      <th>coarse_category</th>\n      <th>pack_count_final</th>\n      <th>total_g</th>\n      <th>total_ml</th>\n      <th>per_item_size</th>\n      <th>...</th>\n      <th>sugar_free</th>\n      <th>premium</th>\n      <th>non_gmo</th>\n      <th>kosher</th>\n      <th>decaf</th>\n      <th>instant</th>\n      <th>refill</th>\n      <th>bulk</th>\n      <th>arabica</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>33127</td>\n      <td>Item Name: La Victoria Green Taco Sauce Mild, ...</td>\n      <td>item name: la victoria green taco sauce mild, ...</td>\n      <td>La Victoria Green Taco Sauce Mild, 12 Ounce (P...</td>\n      <td>La Victoria</td>\n      <td>beverage</td>\n      <td>6.0</td>\n      <td>340.194</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4.89</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>198967</td>\n      <td>Item Name: Salerno Cookies, The Original Butte...</td>\n      <td>item name: salerno cookies, the original butte...</td>\n      <td>Salerno Cookies, The Original Butter Cookies, ...</td>\n      <td>Cookies</td>\n      <td>beverage</td>\n      <td>4.0</td>\n      <td>226.796</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13.12</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 31 columns</p>\n</div>"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"def build_text(df: pd.DataFrame) -> pd.Series:\n    # Title-like phrase + truncated normalized body\n    title = df[\"item_name_phrase\"].fillna(\"\").astype(str)\n    body  = df[\"catalog_norm\"].fillna(\"\").astype(str).str[:500]\n    # Join with a separator token so char ngrams cross less\n    return (title + \" ␟ \" + body).str.strip()\n\ntrain_text = build_text(train)\ntest_text  = build_text(test)\n\nprint(\"Example text:\\n\", train_text.iloc[0][:200])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:18:45.505797Z","iopub.execute_input":"2025-10-12T11:18:45.506049Z","iopub.status.idle":"2025-10-12T11:18:45.846613Z","shell.execute_reply.started":"2025-10-12T11:18:45.506031Z","shell.execute_reply":"2025-10-12T11:18:45.845886Z"}},"outputs":[{"name":"stdout","text":"Example text:\n La Victoria Green Taco Sauce Mild, 12 Ounce (Pack of 6) ␟ item name: la victoria green taco sauce mild, 12 ounce (pack of 6) value: 72.0 unit: fl oz\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"%%time\nfrom sklearn.utils import murmurhash3_32\n\n# Word n-grams\nword_vec = TfidfVectorizer(\n    analyzer=\"word\",\n    ngram_range=(1,2),\n    min_df=3,\n    max_features=150_000,\n    strip_accents=\"unicode\",\n    sublinear_tf=True,\n)\nXw_tr = word_vec.fit_transform(train_text)\nXw_te = word_vec.transform(test_text)\n\n# Char n-grams\nchar_vec = TfidfVectorizer(\n    analyzer=\"char\",\n    ngram_range=(3,5),\n    min_df=10,\n    max_features=120_000,\n    sublinear_tf=True,\n)\nXc_tr = char_vec.fit_transform(train_text)\nXc_te = char_vec.transform(test_text)\n\n# Cast down to float32\nXw_tr = Xw_tr.astype(np.float32); Xw_te = Xw_te.astype(np.float32)\nXc_tr = Xc_tr.astype(np.float32); Xc_te = Xc_te.astype(np.float32)\n\nprint(\"Word tfidf:\", Xw_tr.shape, \"Char tfidf:\", Xc_tr.shape)\ngc.collect();\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:18:45.847381Z","iopub.execute_input":"2025-10-12T11:18:45.847646Z","iopub.status.idle":"2025-10-12T11:20:57.612570Z","shell.execute_reply.started":"2025-10-12T11:18:45.847617Z","shell.execute_reply":"2025-10-12T11:20:57.611798Z"}},"outputs":[{"name":"stdout","text":"Word tfidf: (75000, 150000) Char tfidf: (75000, 120000)\nCPU times: user 2min 8s, sys: 4.01 s, total: 2min 12s\nWall time: 2min 11s\n","output_type":"stream"},{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"30"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"num_cols = [\n    \"log_total_qty_std_final\",\n    \"log_len_words\",\n    \"log_pack_count\",\n    \"len_chars\",\"len_words\",\"num_digits\",\n] + [  # boolean flags we built earlier\n    \"organic\",\"gluten_free\",\"keto\",\"sugar_free\",\"premium\",\"non_gmo\",\"kosher\",\n    \"decaf\",\"instant\",\"refill\",\"bulk\",\"arabica\"\n]\n\n# Ensure the boolean columns exist (if any were missing earlier)\nfor c in num_cols:\n    if c not in train.columns:\n        train[c] = 0\n        test[c]  = 0\n\nnum_tr = train[num_cols].fillna(0.0).astype(np.float32).values\nnum_te = test[num_cols].fillna(0.0).astype(np.float32).values\n\n# Standardize the continuous ones lightly (except pure booleans)\ncont_idx = [num_cols.index(c) for c in [\"log_total_qty_std_final\",\"log_len_words\",\"log_pack_count\",\"len_chars\",\"len_words\",\"num_digits\"]]\nscaler = StandardScaler(with_mean=False)  # with_mean=False for sparse compatibility\nnum_tr_scaled = num_tr.copy()\nnum_te_scaled = num_te.copy()\nnum_tr_scaled[:, cont_idx] = scaler.fit_transform(num_tr[:, cont_idx])\nnum_te_scaled[:, cont_idx] = scaler.transform(num_te[:, cont_idx])\n\nXs_tr = sparse.csr_matrix(num_tr_scaled)\nXs_te = sparse.csr_matrix(num_te_scaled)\n\n# Final design matrix = [word tfidf | char tfidf | small numeric]\nX_tr = sparse.hstack([Xw_tr, Xc_tr, Xs_tr], format=\"csr\")\nX_te = sparse.hstack([Xw_te, Xc_te, Xs_te], format=\"csr\")\n\ndel Xw_tr, Xc_tr, Xs_tr, Xw_te, Xc_te, Xs_te\ngc.collect();\n\nprint(\"Final shapes:\", X_tr.shape, X_te.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:20:57.613556Z","iopub.execute_input":"2025-10-12T11:20:57.613795Z","iopub.status.idle":"2025-10-12T11:20:59.305985Z","shell.execute_reply.started":"2025-10-12T11:20:57.613779Z","shell.execute_reply":"2025-10-12T11:20:59.305182Z"}},"outputs":[{"name":"stdout","text":"Final shapes: (75000, 270018) (75000, 270018)\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"def make_groups(df: pd.DataFrame) -> np.ndarray:\n    # Use item_name_phrase, strip numbers, normalize tokens, keep first ~6 tokens\n    t = df[\"item_name_phrase\"].fillna(\"\").astype(str).str.lower()\n    t = t.str.replace(r\"\\d+\", \" \", regex=True).str.replace(r\"[^a-z]+\", \" \", regex=True)\n    key = t.str.split().str[:6].str.join(\" \")\n    # Hash to int32\n    gids = key.apply(lambda s: int(hashlib.md5(s.encode()).hexdigest()[:8], 16))\n    return gids.values\n\ngroups = make_groups(train)\ny = train[\"price\"].values.astype(np.float32)\ny_log = np.log1p(y)\nprint(\"Groups OK:\", len(groups), \"unique:\", len(np.unique(groups)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:20:59.306903Z","iopub.execute_input":"2025-10-12T11:20:59.307182Z","iopub.status.idle":"2025-10-12T11:20:59.985325Z","shell.execute_reply.started":"2025-10-12T11:20:59.307164Z","shell.execute_reply":"2025-10-12T11:20:59.984707Z"}},"outputs":[{"name":"stdout","text":"Groups OK: 75000 unique: 62039\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"%%time\nALPHAS = [0.05, 0.1, 0.2, 0.5, 1.0]\nFOLDS  = 5\nEPS_FLOOR = 0.10  # to avoid SMAPE blowups on tiny preds\n\ngkf = GroupKFold(n_splits=FOLDS)\n\noof_pred_price = np.zeros(len(train), dtype=np.float32)\ntest_pred_price_folds = []\n\nfold_smapes = []\nfor fold, (tr_idx, va_idx) in enumerate(gkf.split(X_tr, y_log, groups)):\n    Xtr, Xva = X_tr[tr_idx], X_tr[va_idx]\n    ytr, yva = y_log[tr_idx], y_log[va_idx]\n\n    best_alpha, best_smape, best_va_pred = None, 1e9, None\n\n    for a in ALPHAS:\n        # 👇 Force a solver that is stable with sparse matrices on the current SciPy\n        model = Ridge(alpha=a, solver=\"lsqr\", fit_intercept=True, random_state=SEED)\n        model.fit(Xtr, ytr)\n        va_pred_log = model.predict(Xva)\n        va_pred = np.expm1(va_pred_log).astype(np.float32)\n        va_pred = np.clip(va_pred, EPS_FLOOR, None)  # floor to avoid SMAPE blow-ups\n        s = smape(np.expm1(yva), va_pred)\n        if s < best_smape:\n            best_smape = s\n            best_alpha = a\n            best_va_pred = va_pred\n\n    # lock best model for the fold\n    model = Ridge(alpha=best_alpha, solver=\"lsqr\", fit_intercept=True, random_state=SEED)\n    model.fit(Xtr, ytr)\n\n    # Store OOF\n    oof_pred_price[va_idx] = best_va_pred\n\n    # Predict test for this fold\n    te_pred_log = model.predict(X_te)\n    te_pred = np.expm1(te_pred_log).astype(np.float32)\n    te_pred = np.clip(te_pred, EPS_FLOOR, None)\n    test_pred_price_folds.append(te_pred)\n\n    fold_smapes.append(best_smape)\n    print(f\"[Fold {fold}] alpha={best_alpha} SMAPE={best_smape:.4f}\")\n\n# OOF score\noof_smape = smape(y, np.clip(oof_pred_price, EPS_FLOOR, None))\nprint(f\"\\nOOF SMAPE (Ridge TF-IDF): {oof_smape:.4f}\")\n\n# Average test over folds\ntest_pred_price = np.mean(np.vstack(test_pred_price_folds), axis=0).astype(np.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:20:59.986149Z","iopub.execute_input":"2025-10-12T11:20:59.986411Z","iopub.status.idle":"2025-10-12T11:46:21.135250Z","shell.execute_reply.started":"2025-10-12T11:20:59.986391Z","shell.execute_reply":"2025-10-12T11:46:21.134507Z"}},"outputs":[{"name":"stdout","text":"[Fold 0] alpha=1.0 SMAPE=51.7975\n[Fold 1] alpha=1.0 SMAPE=51.6529\n[Fold 2] alpha=1.0 SMAPE=51.9198\n[Fold 3] alpha=1.0 SMAPE=51.6584\n[Fold 4] alpha=1.0 SMAPE=51.7228\n\nOOF SMAPE (Ridge TF-IDF): 51.7503\nCPU times: user 29min 13s, sys: 3.96 s, total: 29min 17s\nWall time: 25min 21s\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"RIDGE_DIR = V2LLM_DIR / \"ridge_tfidf\"\nRIDGE_DIR.mkdir(parents=True, exist_ok=True)\n\nnp.save(RIDGE_DIR/\"oof_price.npy\", oof_pred_price)\nnp.save(RIDGE_DIR/\"test_price.npy\", test_pred_price)\n\n# Save diagnostics\nwith open(RIDGE_DIR/\"oof_metrics.json\", \"w\") as f:\n    json.dump({\n        \"fold_smapes\": [float(s) for s in fold_smapes],\n        \"oof_smape\": float(oof_smape),\n        \"alphas\": ALPHAS\n    }, f, indent=2)\n\n# Optional sanity submission (not final; we'll re-blend later)\nsub = test[[\"sample_id\"]].copy()\nsub[\"price\"] = test_pred_price\nsub_path = RIDGE_DIR / \"baseline_ridge_tfidf_submission.csv\"\nsub.to_csv(sub_path, index=False)\nprint(\"Saved:\")\nprint(\"  OOF preds ->\", RIDGE_DIR/\"oof_price.npy\")\nprint(\"  Test preds->\", RIDGE_DIR/\"test_price.npy\")\nprint(\"  OOF JSON  ->\", RIDGE_DIR/\"oof_metrics.json\")\nprint(\"  Submission->\", sub_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:46:21.136071Z","iopub.execute_input":"2025-10-12T11:46:21.136358Z","iopub.status.idle":"2025-10-12T11:46:21.252101Z","shell.execute_reply.started":"2025-10-12T11:46:21.136334Z","shell.execute_reply":"2025-10-12T11:46:21.251354Z"}},"outputs":[{"name":"stdout","text":"Saved:\n  OOF preds -> /kaggle/working/cache/v2_llm/ridge_tfidf/oof_price.npy\n  Test preds-> /kaggle/working/cache/v2_llm/ridge_tfidf/test_price.npy\n  OOF JSON  -> /kaggle/working/cache/v2_llm/ridge_tfidf/oof_metrics.json\n  Submission-> /kaggle/working/cache/v2_llm/ridge_tfidf/baseline_ridge_tfidf_submission.csv\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"%%capture\n!pip install -U faiss-cpu sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:46:21.255724Z","iopub.execute_input":"2025-10-12T11:46:21.256012Z","iopub.status.idle":"2025-10-12T11:46:27.392568Z","shell.execute_reply.started":"2025-10-12T11:46:21.255994Z","shell.execute_reply":"2025-10-12T11:46:27.391841Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"!pip install -U sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:46:27.393506Z","iopub.execute_input":"2025-10-12T11:46:27.393725Z","iopub.status.idle":"2025-10-12T11:46:30.963783Z","shell.execute_reply.started":"2025-10-12T11:46:27.393705Z","shell.execute_reply":"2025-10-12T11:46:30.963033Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.1.1)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.57.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.35.3)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.15.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"!pip install -U sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:46:30.964909Z","iopub.execute_input":"2025-10-12T11:46:30.965266Z","iopub.status.idle":"2025-10-12T11:46:34.518746Z","shell.execute_reply.started":"2025-10-12T11:46:30.965233Z","shell.execute_reply":"2025-10-12T11:46:34.518037Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.1.1)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.57.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.35.3)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.15.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"import numpy as np, pandas as pd, gc, json, os, re, hashlib, math\nfrom pathlib import Path\nimport torch\nfrom sentence_transformers import SentenceTransformer\n\nSEED = 42\nnp.random.seed(SEED)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nWORK_DIR  = Path(\"/kaggle/working\")\nCACHE_DIR = WORK_DIR / \"cache\"\nV2LLM_DIR = CACHE_DIR / \"v2_llm\"\nKNN_DIR   = V2LLM_DIR / \"knn_faiss\"\nKNN_DIR.mkdir(parents=True, exist_ok=True)\n\n# Load the same feature frames we used for Ridge\ntrain = pd.read_parquet(V2LLM_DIR / \"train_feats_v2_llm.parquet\")\ntest  = pd.read_parquet(V2LLM_DIR / \"test_feats_v2_llm.parquet\")\n\ndef build_embed_text(df: pd.DataFrame) -> list[str]:\n    \"\"\"\n    Short, informative sentence: prefer item_name_phrase + (trusted brand when available).\n    Keep numbers (sizes) — they help match same variants.\n    \"\"\"\n    title = df[\"item_name_phrase\"].fillna(\"\").astype(str)\n    brand = df[\"brand_final\"].fillna(\"\").astype(str)\n    # If brand is generic like \"Organic\"/\"Gourmet\", it won't hurt; MiniLM is robust.\n    txt = (brand.str.strip() + \" || \" + title.str.strip()).str[:256]\n    return txt.tolist()\n\ntrain_sents = build_embed_text(train)\ntest_sents  = build_embed_text(test)\n\nMODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\nembedder = SentenceTransformer(MODEL_NAME, device=device)\n# encode returns float32; normalize=True gives L2-normalized vectors suitable for IP search\nemb_tr = embedder.encode(train_sents, batch_size=1024, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True)\nemb_te = embedder.encode(test_sents,  batch_size=1024, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True)\n\nnp.save(KNN_DIR/\"emb_tr.npy\", emb_tr)\nnp.save(KNN_DIR/\"emb_te.npy\", emb_te)\nprint(\"Embeddings:\", emb_tr.shape, emb_te.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:46:34.519977Z","iopub.execute_input":"2025-10-12T11:46:34.520323Z","iopub.status.idle":"2025-10-12T11:47:37.633406Z","shell.execute_reply.started":"2025-10-12T11:46:34.520291Z","shell.execute_reply":"2025-10-12T11:47:37.632496Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bb95b899c4447dda3d9d3eee53d9e80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1adaf8180d65407cbba29bac60de3017"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b289f5fd15e3449a93c335bf0f17c401"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efd7499618f24dc5a955b52a07052ad5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b7760ce89a14816b6c02bbdbd3fa606"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eda9e355e0aa41ac86b4138eb9d992dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04a8902cffca4457a3ebe11b2b620b74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa43bfe4f453437f98defc0cd1255879"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e851031cbff4069a8493cba3f8fd61e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d20ff12f5ff04e6089a3f569b52dbb4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b81788b5d604ea78511ed1d44f40132"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/74 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58846b82ffe94511ac39103f26c91cce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/74 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9587df126def4d8a95db2d7ddf54b9cc"}},"metadata":{}},{"name":"stdout","text":"Embeddings: (75000, 384) (75000, 384)\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"import faiss\nfrom sklearn.model_selection import GroupKFold\n\ndef smape(y_true, y_pred, eps=1e-9):\n    num = np.abs(y_pred - y_true)\n    den = (np.abs(y_true) + np.abs(y_pred) + eps)/2.0\n    return 100.0 * np.mean(num/den)\n\n# Prepare arrays\ny          = train[\"price\"].values.astype(np.float32)\nqty        = train[\"total_qty_std_final\"].values.astype(np.float32)\nqty_type_s = train[\"qty_type_final\"].fillna(\"unknown\").astype(str).values\nqty_map = {\"ml\":0,\"g\":1,\"count\":2,\"unknown\":3}\nqty_type   = np.array([qty_map.get(v,3) for v in qty_type_s], dtype=np.int8)\n\n# Per-unit price (train only) for rows with known qty\nppu = np.full_like(y, np.nan, dtype=np.float32)\nmask_qty = (qty > 0) & np.isfinite(qty)\nppu[mask_qty] = y[mask_qty] / qty[mask_qty]\n\n# Global anchors (fold-safe usage later)\ndf_tmp = train.copy()\ndf_tmp[\"qty_type_id\"] = qty_type\nglobal_anchor_price = df_tmp.groupby(\"qty_type_id\")[\"price\"].median().to_dict()\nglobal_anchor_ppu   = df_tmp.loc[mask_qty].groupby(\"qty_type_id\").apply(lambda d: np.median(d[\"price\"]/d[\"total_qty_std_final\"])).to_dict()\n\n# Build groups (as in Ridge)\ndef make_groups(df: pd.DataFrame) -> np.ndarray:\n    t = df[\"item_name_phrase\"].fillna(\"\").astype(str).str.lower()\n    t = t.str.replace(r\"\\d+\", \" \", regex=True).str.replace(r\"[^a-z]+\", \" \", regex=True)\n    key = t.str.split().str[:6].str.join(\" \")\n    gids = key.apply(lambda s: int(hashlib.md5(s.encode()).hexdigest()[:8], 16))\n    return gids.values\n\ngroups = make_groups(train)\n\n# Helper: weighted quantiles\ndef weighted_quantile(v, w, qs):\n    v = np.asarray(v, dtype=np.float32)\n    w = np.asarray(w, dtype=np.float32)\n    m = np.isfinite(v) & np.isfinite(w) & (w>0)\n    v = v[m]; w = w[m]\n    if v.size == 0:\n        return [np.nan for _ in qs]\n    order = np.argsort(v)\n    v = v[order]; w = w[order]\n    cw = np.cumsum(w); cw = cw / cw[-1]\n    return [np.interp(q, cw, v) for q in qs]\n\ndef weighted_median(v, w):\n    return weighted_quantile(v, w, [0.5])[0]\n\n# Core retrieval per fold\nK = 100          # retrieve this many neighbors before filtering\nMIN_SIM = 0.35   # discard low-sim neighbors\nMIN_VALID = 5    # need at least this many neighbors after filtering\n\ngkf = GroupKFold(n_splits=5)\n\noof_knn_pred      = np.zeros(len(train), dtype=np.float32)  # main KNN price\noof_knn_pred_rbu  = np.zeros(len(train), dtype=np.float32)  # re-based via per-unit when available\noof_knn_valid_ct  = np.zeros(len(train), dtype=np.int32)    # neighbor count after filtering\n\n# Also collect neighbor stats as features for later LGBM\nfeat_cols = [\"knn_mean\",\"knn_med\",\"knn_p10\",\"knn_p25\",\"knn_p75\",\"knn_p90\",\"knn_std\",\"knn_s1\",\"knn_s2\",\"knn_n\"]\noof_feats = np.zeros((len(train), len(feat_cols)), dtype=np.float32)\n\nfor fold, (tr_idx, va_idx) in enumerate(gkf.split(emb_tr, y, groups)):\n    xb = emb_tr[tr_idx].astype(np.float32).copy()\n    xq = emb_tr[va_idx].astype(np.float32).copy()\n\n    # FAISS IP index (embeddings already L2-normalized)\n    index = faiss.IndexFlatIP(xb.shape[1])\n    index.add(xb)\n\n    sims, idxs = index.search(xq, K)  # (n_val, K)\n    # map local neighbor indices to global train indices\n    idxs_global = tr_idx[idxs]\n\n    # For easy masking\n    qty_type_va = qty_type[va_idx]\n    qty_type_nb = qty_type[idxs_global]\n    price_nb    = y[idxs_global]\n    ppu_nb      = ppu[idxs_global]\n    qty_nb      = qty[idxs_global]\n\n    # For each validation row, compute aggregates\n    for j, vid in enumerate(va_idx):\n        s   = sims[j]\n        nbi = idxs_global[j]\n        # filter by similarity\n        keep = s >= MIN_SIM\n        # same qty_type\n        keep = keep & (qty_type_nb[j] == qty_type_va[j])\n\n        if not np.any(keep):\n            # fallback: no same-type neighbors above threshold → use top K anyway (weak)\n            keep = sims[j] >= np.sort(sims[j])[-MIN_VALID]  # best MIN_VALID\n\n        nbi = nbi[keep]\n        s   = s[keep]\n        prices = price_nb[j][keep]\n\n        # weights\n        w = np.clip(s, 0, 1)**2\n\n        # aggregates (price)\n        med = weighted_median(prices, w)\n        mean = np.average(prices, weights=w) if prices.size>0 else np.nan\n        p10, p25, p75, p90 = weighted_quantile(prices, w, [0.10,0.25,0.75,0.90])\n        std = np.sqrt(np.average((prices - mean)**2, weights=w)) if prices.size>1 and np.isfinite(mean) else 0.0\n        s1 = float(np.max(s)) if s.size>0 else 0.0\n        s2 = float(np.partition(s, -2)[-2]) if s.size>=2 else s1\n        n_valid = int(prices.size)\n\n        # rebase via per-unit if we can (use only neighbors with valid ppu)\n        pred_rbu = np.nan\n        if np.isfinite(train.loc[vid, \"total_qty_std_final\"]) and train.loc[vid, \"total_qty_std_final\"]>0:\n            m_ppu = ppu_nb[j][keep]\n            m_ppu = m_ppu[np.isfinite(m_ppu)]\n            w_ppu = w[np.isfinite(ppu_nb[j][keep])]\n            if m_ppu.size >= MIN_VALID:\n                ppu_med = weighted_median(m_ppu, w_ppu)\n                pred_rbu = ppu_med * float(train.loc[vid, \"total_qty_std_final\"])\n\n        # store\n        oof_knn_pred[vid]     = med if np.isfinite(med) else (global_anchor_price.get(int(qty_type_va[j]), np.median(y)))\n        oof_knn_pred_rbu[vid] = pred_rbu if np.isfinite(pred_rbu) else np.nan\n        oof_knn_valid_ct[vid] = n_valid\n\n        oof_feats[vid] = np.array([mean, med, p10, p25, p75, p90, std, s1, s2, n_valid], dtype=np.float32)\n\n    print(f\"[Fold {fold}] done; avg valid neighbors: {oof_knn_valid_ct[va_idx].mean():.2f}\")\n\n# Choose best of (re-based vs plain) per row\nfinal_oof_knn = np.where(np.isfinite(oof_knn_pred_rbu), oof_knn_pred_rbu, oof_knn_pred)\n\n# SMAPE of KNN OOF\nEPS_FLOOR = 0.10\nknn_oof_smape = smape(y, np.clip(final_oof_knn, EPS_FLOOR, None))\nprint(f\"\\nOOF SMAPE (KNN embeddings): {knn_oof_smape:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:47:37.634385Z","iopub.execute_input":"2025-10-12T11:47:37.635040Z","iopub.status.idle":"2025-10-12T11:48:12.868912Z","shell.execute_reply.started":"2025-10-12T11:47:37.635020Z","shell.execute_reply":"2025-10-12T11:48:12.868265Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_37/2639210051.py:18: RuntimeWarning: invalid value encountered in greater\n  mask_qty = (qty > 0) & np.isfinite(qty)\n/tmp/ipykernel_37/2639210051.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  global_anchor_ppu   = df_tmp.loc[mask_qty].groupby(\"qty_type_id\").apply(lambda d: np.median(d[\"price\"]/d[\"total_qty_std_final\"])).to_dict()\n","output_type":"stream"},{"name":"stdout","text":"[Fold 0] done; avg valid neighbors: 61.08\n[Fold 1] done; avg valid neighbors: 61.67\n[Fold 2] done; avg valid neighbors: 61.47\n[Fold 3] done; avg valid neighbors: 60.72\n[Fold 4] done; avg valid neighbors: 61.28\n\nOOF SMAPE (KNN embeddings): 74.9601\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"# Build one full-train index for test inference\nindex = faiss.IndexFlatIP(emb_tr.shape[1])\nindex.add(emb_tr)\n\nK = 100\nMIN_SIM = 0.35\ntest_sims, test_idxs = index.search(emb_te, K)\nqty_type_te = np.array([qty_map.get(v,3) for v in test[\"qty_type_final\"].fillna(\"unknown\").astype(str).values], dtype=np.int8)\n\n# Precompute arrays for convenience\nprice_tr  = y\nppu_tr    = ppu\nqty_tr    = qty\nqtytype_tr= qty_type\n\ntest_pred_med   = np.zeros(len(test), dtype=np.float32)\ntest_pred_rbu   = np.full(len(test), np.nan, dtype=np.float32)\ntest_feat_mat   = np.zeros((len(test), len(feat_cols)), dtype=np.float32)\ntest_valid_ct   = np.zeros(len(test), dtype=np.int32)\n\nfor i in range(len(test)):\n    s = test_sims[i]\n    gi = test_idxs[i]  # indices into train\n    # filter by similarity and same qty_type\n    keep = (s >= MIN_SIM) & (qtytype_tr[gi] == qty_type_te[i])\n    if not np.any(keep):\n        keep = s >= np.sort(s)[-min(MIN_VALID, s.size)]\n    gi = gi[keep]; s = s[keep]\n    prices = price_tr[gi]\n    w = np.clip(s, 0, 1)**2\n\n    # aggregates\n    med = weighted_median(prices, w)\n    mean = np.average(prices, weights=w) if prices.size>0 else np.nan\n    p10, p25, p75, p90 = weighted_quantile(prices, w, [0.10,0.25,0.75,0.90])\n    std = np.sqrt(np.average((prices - mean)**2, weights=w)) if prices.size>1 and np.isfinite(mean) else 0.0\n    s1 = float(np.max(s)) if s.size>0 else 0.0\n    s2 = float(np.partition(s, -2)[-2]) if s.size>=2 else s1\n    n_valid = int(prices.size)\n\n    test_pred_med[i] = med if np.isfinite(med) else float(np.median(y))\n    test_valid_ct[i] = n_valid\n    test_feat_mat[i] = np.array([mean, med, p10, p25, p75, p90, std, s1, s2, n_valid], dtype=np.float32)\n\n    # re-base via per-unit when test qty is known\n    tqty = test.loc[i, \"total_qty_std_final\"]\n    if np.isfinite(tqty) and tqty>0:\n        m_ppu = ppu_tr[gi]\n        m_ppu = m_ppu[np.isfinite(m_ppu)]\n        w_ppu = w[np.isfinite(ppu_tr[gi])]\n        if m_ppu.size >= MIN_VALID:\n            ppu_med = weighted_median(m_ppu, w_ppu)\n            test_pred_rbu[i] = ppu_med * float(tqty)\n\nfinal_test_knn = np.where(np.isfinite(test_pred_rbu), test_pred_rbu, test_pred_med)\n\nprint(\"Test KNN predictions computed:\", len(final_test_knn))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:48:12.869671Z","iopub.execute_input":"2025-10-12T11:48:12.869862Z","iopub.status.idle":"2025-10-12T11:48:47.129262Z","shell.execute_reply.started":"2025-10-12T11:48:12.869844Z","shell.execute_reply":"2025-10-12T11:48:47.128550Z"}},"outputs":[{"name":"stdout","text":"Test KNN predictions computed: 75000\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"# OOF/test predictions\nnp.save(KNN_DIR/\"oof_knn_price.npy\", final_oof_knn.astype(np.float32))\nnp.save(KNN_DIR/\"test_knn_price.npy\", final_test_knn.astype(np.float32))\n\n# Neighbor features for later LightGBM\noof_knn_feats_df  = pd.DataFrame(oof_feats, columns=feat_cols)\noof_knn_feats_df.insert(0, \"sample_id\", train[\"sample_id\"].values)\noof_knn_feats_df.to_parquet(KNN_DIR/\"oof_knn_features.parquet\", index=False)\n\ntest_knn_feats_df = pd.DataFrame(test_feat_mat, columns=feat_cols)\ntest_knn_feats_df.insert(0, \"sample_id\", test[\"sample_id\"].values)\ntest_knn_feats_df.to_parquet(KNN_DIR/\"test_knn_features.parquet\", index=False)\n\n# Quick submission from KNN alone (for sanity)\nsub_knn = test[[\"sample_id\"]].copy()\nsub_knn[\"price\"] = final_test_knn\nsub_knn_path = KNN_DIR/\"baseline_knn_submission.csv\"\nsub_knn.to_csv(sub_knn_path, index=False)\n\nprint(\"Saved:\")\nprint(\"  OOF  ->\", KNN_DIR/\"oof_knn_price.npy\")\nprint(\"  TEST ->\", KNN_DIR/\"test_knn_price.npy\")\nprint(\"  OOF  features ->\", KNN_DIR/\"oof_knn_features.parquet\")\nprint(\"  TEST features ->\", KNN_DIR/\"test_knn_features.parquet\")\nprint(\"  KNN submission ->\", sub_knn_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:48:47.130043Z","iopub.execute_input":"2025-10-12T11:48:47.130316Z","iopub.status.idle":"2025-10-12T11:48:47.357067Z","shell.execute_reply.started":"2025-10-12T11:48:47.130289Z","shell.execute_reply":"2025-10-12T11:48:47.356271Z"}},"outputs":[{"name":"stdout","text":"Saved:\n  OOF  -> /kaggle/working/cache/v2_llm/knn_faiss/oof_knn_price.npy\n  TEST -> /kaggle/working/cache/v2_llm/knn_faiss/test_knn_price.npy\n  OOF  features -> /kaggle/working/cache/v2_llm/knn_faiss/oof_knn_features.parquet\n  TEST features -> /kaggle/working/cache/v2_llm/knn_faiss/test_knn_features.parquet\n  KNN submission -> /kaggle/working/cache/v2_llm/knn_faiss/baseline_knn_submission.csv\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"# Load ridge oof saved earlier\nRIDGE_DIR = V2LLM_DIR / \"ridge_tfidf\"\nridge_oof = np.load(RIDGE_DIR/\"oof_price.npy\")\n\nEPS_FLOOR = 0.10\nprint(\"Ridge OOF SMAPE: \", smape(y, np.clip(ridge_oof, EPS_FLOOR, None)).round(4))\nprint(\"KNN   OOF SMAPE: \", smape(y, np.clip(final_oof_knn, EPS_FLOOR, None)).round(4))\n\n# Naive blend (50/50) just to see if it improves (we'll do proper stacking later)\nblend_oof = 0.5*ridge_oof + 0.5*final_oof_knn\nprint(\"Naive 50/50 blend OOF SMAPE:\", smape(y, np.clip(blend_oof, EPS_FLOOR, None)).round(4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:48:47.357994Z","iopub.execute_input":"2025-10-12T11:48:47.358225Z","iopub.status.idle":"2025-10-12T11:48:47.366842Z","shell.execute_reply.started":"2025-10-12T11:48:47.358208Z","shell.execute_reply":"2025-10-12T11:48:47.366089Z"}},"outputs":[{"name":"stdout","text":"Ridge OOF SMAPE:  51.7503\nKNN   OOF SMAPE:  74.9601\nNaive 50/50 blend OOF SMAPE: 57.6303\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"print(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:48:47.367731Z","iopub.execute_input":"2025-10-12T11:48:47.367988Z","iopub.status.idle":"2025-10-12T11:48:47.383596Z","shell.execute_reply.started":"2025-10-12T11:48:47.367968Z","shell.execute_reply":"2025-10-12T11:48:47.383059Z"}},"outputs":[{"name":"stdout","text":"5\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"%%capture\n!pip install -U lightgbm\n\nimport os, gc, re, json, hashlib, math\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom collections import Counter, defaultdict\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import LabelEncoder\n\nSEED = 42\nnp.random.seed(SEED)\n\ndef smape(y_true, y_pred, eps=1e-9):\n    num = np.abs(y_pred - y_pred)\n    den = (np.abs(y_true) + np.abs(y_pred) + eps)/2.0\n    return 100.0 * np.mean(num/den)\n\n# OOPS: fixed version (typo above—keep this one!)\ndef smape(y_true, y_pred, eps=1e-9):\n    num = np.abs(y_pred - y_true)\n    den = (np.abs(y_true) + np.abs(y_pred) + eps)/2.0\n    return 100.0 * np.mean(num/den)\n\nWORK_DIR  = Path(\"/kaggle/working\")\nCACHE_DIR = WORK_DIR / \"cache\"\nV2LLM_DIR = CACHE_DIR / \"v2_llm\"\nKNN_DIR   = V2LLM_DIR / \"knn_faiss\"\nLGB_DIR   = V2LLM_DIR / \"lightgbm\"\nLGB_DIR.mkdir(parents=True, exist_ok=True)\n\ntrain = pd.read_parquet(V2LLM_DIR/\"train_feats_v2_llm.parquet\")\ntest  = pd.read_parquet(V2LLM_DIR/\"test_feats_v2_llm.parquet\")\n\n# Load embeddings (already saved) for clustering\nemb_tr = np.load(KNN_DIR/\"emb_tr.npy\")\nemb_te = np.load(KNN_DIR/\"emb_te.npy\")\n\n# Load neighbor features + OOF preds we saved earlier\noof_knn_feats = pd.read_parquet(KNN_DIR/\"oof_knn_features.parquet\")\ntest_knn_feats= pd.read_parquet(KNN_DIR/\"test_knn_features.parquet\")\nridge_oof     = np.load(V2LLM_DIR/\"ridge_tfidf/oof_price.npy\")\nridge_test    = np.load(V2LLM_DIR/\"ridge_tfidf/test_price.npy\")\nknn_oof       = np.load(KNN_DIR/\"oof_knn_price.npy\")\nknn_test      = np.load(KNN_DIR/\"test_knn_price.npy\")\n\n# Ensure alignment by sample_id\nassert (oof_knn_feats[\"sample_id\"].values == train[\"sample_id\"].values).all()\nassert (test_knn_feats[\"sample_id\"].values == test[\"sample_id\"].values).all()\n\n# Attach meta preds + knn feats\ntrain[\"ridge_oof\"] = ridge_oof\ntrain[\"knn_oof\"]   = knn_oof\ntest[\"ridge_test\"] = ridge_test\ntest[\"knn_test\"]   = knn_test\n\ntrain = train.merge(oof_knn_feats, on=\"sample_id\", how=\"left\")\ntest  = test.merge(test_knn_feats, on=\"sample_id\", how=\"left\")\n\nprint(train.shape, test.shape)\ntrain.head(3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:48:47.384193Z","iopub.execute_input":"2025-10-12T11:48:47.384345Z","iopub.status.idle":"2025-10-12T11:48:57.183191Z","shell.execute_reply.started":"2025-10-12T11:48:47.384333Z","shell.execute_reply":"2025-10-12T11:48:57.182227Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\ndef make_groups(df: pd.DataFrame) -> np.ndarray:\n    t = df[\"item_name_phrase\"].fillna(\"\").astype(str).str.lower()\n    t = t.str.replace(r\"\\d+\", \" \", regex=True).str.replace(r\"[^a-z]+\", \" \", regex=True)\n    key = t.str.split().str[:6].str.join(\" \")\n    gids = key.apply(lambda s: int(hashlib.md5(s.encode()).hexdigest()[:8], 16))\n    return gids.values\n\ngroups = make_groups(train)\n\nK_CLUST = 128  # modest; good tradeoff\nkm = KMeans(n_clusters=K_CLUST, random_state=SEED, n_init=\"auto\", max_iter=200)\nembed_cluster_tr = km.fit_predict(emb_tr)\nembed_cluster_te = km.predict(emb_te)\n\ntrain[\"embed_cluster\"] = embed_cluster_tr.astype(np.int32)\ntest[\"embed_cluster\"]  = embed_cluster_te.astype(np.int32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:48:57.184332Z","iopub.execute_input":"2025-10-12T11:48:57.185171Z","iopub.status.idle":"2025-10-12T11:49:13.824820Z","shell.execute_reply.started":"2025-10-12T11:48:57.185136Z","shell.execute_reply":"2025-10-12T11:49:13.824169Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"# Prepare core arrays\ny = train[\"price\"].values.astype(np.float32)\n\nqty_tr = train[\"total_qty_std_final\"].astype(float).values\nqty_te = test[\"total_qty_std_final\"].astype(float).values\nhas_qty_tr = np.isfinite(qty_tr) & (qty_tr>0)\nhas_qty_te = np.isfinite(qty_te) & (qty_te>0)\nppu_tr = np.full_like(y, np.nan, dtype=np.float32)\nppu_tr[has_qty_tr] = y[has_qty_tr] / qty_tr[has_qty_tr]\n\n# Helper to compute fold-safe medians\ndef foldwise_median_map(keys, values, tr_idx):\n    \"\"\"Return dict: key -> median (computed only on tr_idx).\"\"\"\n    d = {}\n    tmp = pd.DataFrame({\"k\":keys[tr_idx], \"v\":values[tr_idx]})\n    g = tmp.groupby(\"k\")[\"v\"].median()\n    d = g.to_dict()\n    med_all = np.nanmedian(values[tr_idx])\n    return d, float(med_all)\n\ndef apply_map(keys, d, default):\n    return np.array([d.get(k, default) for k in keys], dtype=np.float32)\n\n# Categorical keys\nbrand_key_tr = train[\"brand_final\"].astype(str).values\nbrand_key_te = test[\"brand_final\"].astype(str).values\ncat_key_tr   = train[\"coarse_category\"].astype(str).values\ncat_key_te   = test[\"coarse_category\"].astype(str).values\nqtytype_tr   = train[\"qty_type_final\"].astype(str).values\nqtytype_te   = test[\"qty_type_final\"].astype(str).values\ncluster_tr   = train[\"embed_cluster\"].astype(int).values\ncluster_te   = test[\"embed_cluster\"].astype(int).values\n\nFOLDS = 5\ngkf = GroupKFold(n_splits=FOLDS)\n\n# Placeholders for fold-safe features\nfold_te_cols = [\n    \"te_brand_price\",\"te_brand_ppu\",\n    \"te_cluster_ppu\",\"te_cat_ppu\",\"te_qtytype_ppu\"\n]\noof_te_feats = {c: np.zeros(len(train), dtype=np.float32) * np.nan for c in fold_te_cols}\n\n# For test we will use medians computed on FULL train\nfull_brand_price_med = pd.DataFrame({\"k\":brand_key_tr, \"v\":y}).groupby(\"k\")[\"v\"].median().to_dict()\nfull_brand_ppu_med   = pd.DataFrame({\"k\":brand_key_tr[has_qty_tr], \"v\":ppu_tr[has_qty_tr]}).groupby(\"k\")[\"v\"].median().to_dict()\nfull_cluster_ppu_med = pd.DataFrame({\"k\":cluster_tr[has_qty_tr], \"v\":ppu_tr[has_qty_tr]}).groupby(\"k\")[\"v\"].median().to_dict()\nfull_cat_ppu_med     = pd.DataFrame({\"k\":cat_key_tr[has_qty_tr], \"v\":ppu_tr[has_qty_tr]}).groupby(\"k\")[\"v\"].median().to_dict()\nfull_qtytype_ppu_med = pd.DataFrame({\"k\":qtytype_tr[has_qty_tr], \"v\":ppu_tr[has_qty_tr]}).groupby(\"k\")[\"v\"].median().to_dict()\n\n# Global fallbacks\nglobal_price_med = float(np.median(y))\nglobal_ppu_med   = float(np.nanmedian(ppu_tr))\n\nfor fold, (tr_idx, va_idx) in enumerate(gkf.split(train, y, groups)):\n    # brand price / ppu\n    m_brand_price, def_bp = foldwise_median_map(brand_key_tr, y, tr_idx)\n    m_brand_ppu,   def_bu = foldwise_median_map(brand_key_tr, ppu_tr, tr_idx)\n\n    # cluster ppu\n    m_cluster_ppu, def_cp = foldwise_median_map(cluster_tr, ppu_tr, tr_idx)\n\n    # category ppu\n    m_cat_ppu, def_cat = foldwise_median_map(cat_key_tr, ppu_tr, tr_idx)\n\n    # qtytype ppu\n    m_qtytype_ppu, def_qt = foldwise_median_map(qtytype_tr, ppu_tr, tr_idx)\n\n    oof_te_feats[\"te_brand_price\"][va_idx]  = apply_map(brand_key_tr[va_idx],   m_brand_price, def_bp if not math.isnan(def_bp) else global_price_med)\n    oof_te_feats[\"te_brand_ppu\"][va_idx]    = apply_map(brand_key_tr[va_idx],   m_brand_ppu,   def_bu if not math.isnan(def_bu) else global_ppu_med)\n    oof_te_feats[\"te_cluster_ppu\"][va_idx]  = apply_map(cluster_tr[va_idx],     m_cluster_ppu, def_cp if not math.isnan(def_cp) else global_ppu_med)\n    oof_te_feats[\"te_cat_ppu\"][va_idx]      = apply_map(cat_key_tr[va_idx],     m_cat_ppu,     def_cat if not math.isnan(def_cat) else global_ppu_med)\n    oof_te_feats[\"te_qtytype_ppu\"][va_idx]  = apply_map(qtytype_tr[va_idx],     m_qtytype_ppu, def_qt if not math.isnan(def_qt) else global_ppu_med)\n\n# Assemble OOF TE features\nfor k,v in oof_te_feats.items():\n    train[k] = v\n\n# Test TE features from FULL medians\ntest[\"te_brand_price\"]  = apply_map(brand_key_te, full_brand_price_med, global_price_med)\ntest[\"te_brand_ppu\"]    = apply_map(brand_key_te, full_brand_ppu_med,   global_ppu_med)\ntest[\"te_cluster_ppu\"]  = apply_map(cluster_te,   full_cluster_ppu_med, global_ppu_med)\ntest[\"te_cat_ppu\"]      = apply_map(cat_key_te,   full_cat_ppu_med,     global_ppu_med)\ntest[\"te_qtytype_ppu\"]  = apply_map(qtytype_te,   full_qtytype_ppu_med, global_ppu_med)\n\n# Anchor prices (multiply ppu medians by qty)\ndef anchor(ppu_arr, qty_arr):\n    out = np.full_like(qty_arr, np.nan, dtype=np.float32)\n    m = np.isfinite(ppu_arr) & np.isfinite(qty_arr) & (qty_arr>0)\n    out[m] = ppu_arr[m] * qty_arr[m]\n    return out\n\nfor pref in [\"brand\",\"cluster\",\"cat\",\"qtytype\"]:\n    train[f\"anchor_{pref}\"] = anchor(train[f\"te_{pref}_ppu\"].values, qty_tr)\n    test[f\"anchor_{pref}\"]  = anchor(test[f\"te_{pref}_ppu\"].values,  qty_te)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:49:13.825713Z","iopub.execute_input":"2025-10-12T11:49:13.825958Z","iopub.status.idle":"2025-10-12T11:49:14.349566Z","shell.execute_reply.started":"2025-10-12T11:49:13.825932Z","shell.execute_reply":"2025-10-12T11:49:14.348611Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_37/1953632452.py:6: RuntimeWarning: invalid value encountered in greater\n  has_qty_tr = np.isfinite(qty_tr) & (qty_tr>0)\n/tmp/ipykernel_37/1953632452.py:7: RuntimeWarning: invalid value encountered in greater\n  has_qty_te = np.isfinite(qty_te) & (qty_te>0)\n/tmp/ipykernel_37/1953632452.py:89: RuntimeWarning: invalid value encountered in greater\n  m = np.isfinite(ppu_arr) & np.isfinite(qty_arr) & (qty_arr>0)\n/tmp/ipykernel_37/1953632452.py:89: RuntimeWarning: invalid value encountered in greater\n  m = np.isfinite(ppu_arr) & np.isfinite(qty_arr) & (qty_arr>0)\n/tmp/ipykernel_37/1953632452.py:89: RuntimeWarning: invalid value encountered in greater\n  m = np.isfinite(ppu_arr) & np.isfinite(qty_arr) & (qty_arr>0)\n/tmp/ipykernel_37/1953632452.py:89: RuntimeWarning: invalid value encountered in greater\n  m = np.isfinite(ppu_arr) & np.isfinite(qty_arr) & (qty_arr>0)\n/tmp/ipykernel_37/1953632452.py:89: RuntimeWarning: invalid value encountered in greater\n  m = np.isfinite(ppu_arr) & np.isfinite(qty_arr) & (qty_arr>0)\n/tmp/ipykernel_37/1953632452.py:89: RuntimeWarning: invalid value encountered in greater\n  m = np.isfinite(ppu_arr) & np.isfinite(qty_arr) & (qty_arr>0)\n/tmp/ipykernel_37/1953632452.py:89: RuntimeWarning: invalid value encountered in greater\n  m = np.isfinite(ppu_arr) & np.isfinite(qty_arr) & (qty_arr>0)\n/tmp/ipykernel_37/1953632452.py:89: RuntimeWarning: invalid value encountered in greater\n  m = np.isfinite(ppu_arr) & np.isfinite(qty_arr) & (qty_arr>0)\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"# Map brand to top-K, others -> \"__other__\" to keep cardinality tame\nTOPK_BRANDS = 1000\ntop_brands = [b for b,_ in Counter(train[\"brand_final\"].astype(str)).most_common(TOPK_BRANDS)]\ntrain[\"brand_top\"] = np.where(train[\"brand_final\"].isin(top_brands), train[\"brand_final\"], \"__other__\")\ntest[\"brand_top\"]  = np.where(test[\"brand_final\"].isin(top_brands),  test[\"brand_final\"],  \"__other__\")\n\n# Feature lists\nnum_cols = [\n    \"log_total_qty_std_final\",\"log_len_words\",\"log_pack_count\",\n    \"len_chars\",\"len_words\",\"num_digits\",\n    # neighbor stats\n    \"knn_mean\",\"knn_med\",\"knn_p10\",\"knn_p25\",\"knn_p75\",\"knn_p90\",\"knn_std\",\"knn_s1\",\"knn_s2\",\"knn_n\",\n    # TE + anchors\n    \"te_brand_price\",\"te_brand_ppu\",\"te_cluster_ppu\",\"te_cat_ppu\",\"te_qtytype_ppu\",\n    \"anchor_brand\",\"anchor_cluster\",\"anchor_cat\",\"anchor_qtytype\",\n    # meta preds (OOF/test)\n    \"ridge_oof\",\"knn_oof\",\n]\nnum_cols_test = [c.replace(\"_oof\",\"_test\") if c.endswith(\"_oof\") else c for c in num_cols]\n\n# Booleans\nbool_cols = [\"organic\",\"gluten_free\",\"keto\",\"sugar_free\",\"premium\",\"non_gmo\",\"kosher\",\"decaf\",\"instant\",\"refill\",\"bulk\",\"arabica\"]\nfor c in bool_cols:\n    if c not in train.columns: train[c]=0\n    if c not in test.columns:  test[c]=0\nnum_cols += bool_cols\nnum_cols_test += bool_cols\n\n# Categorical columns\ncat_cols = [\"brand_top\",\"coarse_category\",\"qty_type_final\",\"embed_cluster\"]\n\n# Build X/y\nX_num  = train[num_cols].fillna(0.0).astype(np.float32)\nXc     = train[cat_cols].copy()\nXt_num = test[num_cols_test].fillna(0.0).astype(np.float32)\nXtc    = test[cat_cols].copy()\n\n# Label encode categoricals and keep as category dtype\ncat_map = {}\nfor c in cat_cols:\n    le = LabelEncoder()\n    all_vals = pd.concat([Xc[c].astype(str), Xtc[c].astype(str)], axis=0)\n    le.fit(all_vals)\n    Xc[c]  = pd.Categorical(le.transform(Xc[c].astype(str)))\n    Xtc[c] = pd.Categorical(le.transform(Xtc[c].astype(str)))\n    cat_map[c] = le\n\n# Final design frames\nX_train = pd.concat([X_num, Xc], axis=1)\nX_test  = pd.concat([Xt_num, Xtc], axis=1)\n\n# LightGBM understands pandas categorical dtype\ny = train[\"price\"].values.astype(np.float32)\n\nprint(\"Train/Test shapes:\", X_train.shape, X_test.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:49:14.350447Z","iopub.execute_input":"2025-10-12T11:49:14.350693Z","iopub.status.idle":"2025-10-12T11:49:14.630824Z","shell.execute_reply.started":"2025-10-12T11:49:14.350667Z","shell.execute_reply":"2025-10-12T11:49:14.630066Z"}},"outputs":[{"name":"stdout","text":"Train/Test shapes: (75000, 43) (75000, 43)\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"def smape_weight_scheme(y):\n    return 1.0 / np.sqrt(np.clip(y, 1e-2, None))\n\nparams = dict(\n    objective=\"regression_l1\",\n    metric=\"mae\",\n    learning_rate=0.05,\n    num_leaves=64,\n    max_depth=-1,\n    min_data_in_leaf=40,\n    feature_fraction=0.9,\n    bagging_fraction=0.8,\n    bagging_freq=1,\n    lambda_l1=0.0,\n    lambda_l2=1.0,\n    verbosity=-1,\n    seed=SEED,\n)\n\nFOLDS = 5\ngkf = GroupKFold(n_splits=FOLDS)\n\noof_lgb = np.zeros(len(X_train), dtype=np.float32)\npreds_test_folds = []\nfold_metrics = []\n\nweights = smape_weight_scheme(y)\n\nfor fold, (tr_idx, va_idx) in enumerate(gkf.split(X_train, y, groups)):\n    dtr = lgb.Dataset(X_train.iloc[tr_idx], label=y[tr_idx], weight=weights[tr_idx], categorical_feature=cat_cols, free_raw_data=False)\n    dva = lgb.Dataset(X_train.iloc[va_idx], label=y[va_idx], weight=weights[va_idx], categorical_feature=cat_cols, free_raw_data=False)\n\n    model = lgb.train(\n        params,\n        dtr,\n        num_boost_round=5000,\n        valid_sets=[dtr, dva],\n        valid_names=[\"tr\",\"va\"],\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=200, verbose=False),\n            lgb.log_evaluation(period=200),\n        ],\n    )\n\n    va_pred = model.predict(X_train.iloc[va_idx], num_iteration=model.best_iteration).astype(np.float32)\n    oof_lgb[va_idx] = va_pred\n\n    te_pred = model.predict(X_test, num_iteration=model.best_iteration).astype(np.float32)\n    preds_test_folds.append(te_pred)\n\n    score = smape(y[va_idx], np.clip(va_pred, 0.10, None))\n    fold_metrics.append(score)\n    print(f\"[Fold {fold}] best_iter={model.best_iteration} SMAPE={score:.4f}\")\n\noof_smape = smape(y, np.clip(oof_lgb, 0.10, None))\nprint(f\"\\nOOF SMAPE (LightGBM): {oof_smape:.4f}\")\npreds_lgb_test = np.mean(np.vstack(preds_test_folds), axis=0).astype(np.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:49:14.631673Z","iopub.execute_input":"2025-10-12T11:49:14.631891Z","iopub.status.idle":"2025-10-12T11:52:06.215829Z","shell.execute_reply.started":"2025-10-12T11:49:14.631870Z","shell.execute_reply":"2025-10-12T11:52:06.215091Z"}},"outputs":[{"name":"stdout","text":"[200]\ttr's l1: 6.00844\tva's l1: 6.71909\n[400]\ttr's l1: 5.84117\tva's l1: 6.71006\n[Fold 0] best_iter=327 SMAPE=51.4214\n[200]\ttr's l1: 6.03331\tva's l1: 6.5895\n[400]\ttr's l1: 5.86663\tva's l1: 6.57698\n[600]\ttr's l1: 5.76088\tva's l1: 6.57166\n[800]\ttr's l1: 5.69085\tva's l1: 6.57089\n[1000]\ttr's l1: 5.6334\tva's l1: 6.56857\n[1200]\ttr's l1: 5.58655\tva's l1: 6.56701\n[1400]\ttr's l1: 5.54121\tva's l1: 6.5659\n[1600]\ttr's l1: 5.50521\tva's l1: 6.56604\n[Fold 1] best_iter=1539 SMAPE=50.4869\n[200]\ttr's l1: 6.01139\tva's l1: 6.70761\n[400]\ttr's l1: 5.8516\tva's l1: 6.69818\n[600]\ttr's l1: 5.74509\tva's l1: 6.692\n[800]\ttr's l1: 5.66699\tva's l1: 6.69039\n[1000]\ttr's l1: 5.60443\tva's l1: 6.6909\n[1200]\ttr's l1: 5.55578\tva's l1: 6.68983\n[1400]\ttr's l1: 5.5166\tva's l1: 6.69201\n[Fold 2] best_iter=1216 SMAPE=51.2191\n[200]\ttr's l1: 6.01188\tva's l1: 6.685\n[400]\ttr's l1: 5.84953\tva's l1: 6.67726\n[600]\ttr's l1: 5.74959\tva's l1: 6.67304\n[800]\ttr's l1: 5.67637\tva's l1: 6.6704\n[1000]\ttr's l1: 5.61982\tva's l1: 6.66937\n[1200]\ttr's l1: 5.57076\tva's l1: 6.66869\n[1400]\ttr's l1: 5.52819\tva's l1: 6.66699\n[Fold 3] best_iter=1359 SMAPE=51.2392\n[200]\ttr's l1: 6.03854\tva's l1: 6.62703\n[400]\ttr's l1: 5.87342\tva's l1: 6.62064\n[600]\ttr's l1: 5.77061\tva's l1: 6.62091\n[Fold 4] best_iter=525 SMAPE=51.7255\n\nOOF SMAPE (LightGBM): 51.2184\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"def train_quantile(alpha):\n    qparams = params.copy()\n    qparams.update(dict(objective=\"quantile\", alpha=alpha, metric=\"quantile\"))\n    oof_q = np.zeros(len(X_train), dtype=np.float32)\n    preds_q_folds = []\n    for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_train, y, groups)):\n        dtr = lgb.Dataset(X_train.iloc[tr_idx], label=y[tr_idx], weight=weights[tr_idx], categorical_feature=cat_cols, free_raw_data=False)\n        dva = lgb.Dataset(X_train.iloc[va_idx], label=y[va_idx], weight=weights[va_idx], categorical_feature=cat_cols, free_raw_data=False)\n        mdl = lgb.train(\n            qparams, dtr, num_boost_round=2000, valid_sets=[dtr, dva],\n            callbacks=[lgb.early_stopping(200, verbose=False)]\n        )\n        oof_q[va_idx] = mdl.predict(X_train.iloc[va_idx], num_iteration=mdl.best_iteration).astype(np.float32)\n        preds_q_folds.append(mdl.predict(X_test, num_iteration=mdl.best_iteration).astype(np.float32))\n    pred_q_test = np.mean(np.vstack(preds_q_folds), axis=0).astype(np.float32)\n    return oof_q, pred_q_test\n\nprint(\"Training quantile p10 ...\")\noof_p10, te_p10 = train_quantile(alpha=0.10)\nprint(\"Training quantile p90 ...\")\noof_p90, te_p90 = train_quantile(alpha=0.90)\n\n# Clip LGB predictions to [p10,p90]\noof_lgb_clipped = np.clip(oof_lgb, a_min=oof_p10, a_max=oof_p90)\ntest_lgb_clipped = np.clip(preds_lgb_test, a_min=te_p10, a_max=te_p90)\n\nprint(\"OOF SMAPE (LGB clipped):\", smape(y, np.clip(oof_lgb_clipped, 0.10, None)).round(4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:52:06.216691Z","iopub.execute_input":"2025-10-12T11:52:06.217413Z","iopub.status.idle":"2025-10-12T11:53:46.093446Z","shell.execute_reply.started":"2025-10-12T11:52:06.217388Z","shell.execute_reply":"2025-10-12T11:53:46.092613Z"}},"outputs":[{"name":"stdout","text":"Training quantile p10 ...\nTraining quantile p90 ...\nOOF SMAPE (LGB clipped): 51.1569\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"np.save(LGB_DIR/\"oof_lgb.npy\", oof_lgb)\nnp.save(LGB_DIR/\"test_lgb.npy\", preds_lgb_test)\nnp.save(LGB_DIR/\"oof_lgb_clipped.npy\", oof_lgb_clipped if 'oof_lgb_clipped' in locals() else oof_lgb)\nnp.save(LGB_DIR/\"test_lgb_clipped.npy\", test_lgb_clipped if 'test_lgb_clipped' in locals() else preds_lgb_test)\n\nwith open(LGB_DIR/\"oof_metrics.json\", \"w\") as f:\n    json.dump({\n        \"fold_smapes\": [float(s) for s in fold_metrics],\n        \"oof_smape\": float(oof_smape)\n    }, f, indent=2)\n\n# quick submission (we will re-blend in next phase)\nsub_lgb = test[[\"sample_id\"]].copy()\nsub_lgb[\"price\"] = test_lgb_clipped if 'test_lgb_clipped' in locals() else preds_lgb_test\nsub_lgb_path = LGB_DIR/\"baseline_lgb_submission.csv\"\nsub_lgb.to_csv(sub_lgb_path, index=False)\nprint(\"Saved:\", sub_lgb_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:53:46.094367Z","iopub.execute_input":"2025-10-12T11:53:46.094585Z","iopub.status.idle":"2025-10-12T11:53:46.202634Z","shell.execute_reply.started":"2025-10-12T11:53:46.094570Z","shell.execute_reply":"2025-10-12T11:53:46.202033Z"}},"outputs":[{"name":"stdout","text":"Saved: /kaggle/working/cache/v2_llm/lightgbm/baseline_lgb_submission.csv\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"df= pd.read_csv('/kaggle/working/cache/v2_llm/lightgbm/baseline_lgb_submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:59:04.252041Z","iopub.execute_input":"2025-10-12T11:59:04.252308Z","iopub.status.idle":"2025-10-12T11:59:04.273062Z","shell.execute_reply.started":"2025-10-12T11:59:04.252288Z","shell.execute_reply":"2025-10-12T11:59:04.272482Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T11:59:09.766182Z","iopub.execute_input":"2025-10-12T11:59:09.766449Z","iopub.status.idle":"2025-10-12T11:59:09.773867Z","shell.execute_reply.started":"2025-10-12T11:59:09.766428Z","shell.execute_reply":"2025-10-12T11:59:09.773203Z"}},"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"   sample_id      price\n0     100179  11.714991\n1     245611  10.738874\n2     146263  16.404509\n3      95658   4.567592\n4      36806  27.749157","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100179</td>\n      <td>11.714991</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>245611</td>\n      <td>10.738874</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>146263</td>\n      <td>16.404509</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>95658</td>\n      <td>4.567592</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>36806</td>\n      <td>27.749157</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":73},{"cell_type":"code","source":"df.to_csv('my_dataframe.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}